{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca4b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pprint\n",
    "from itertools import product, islice\n",
    "import tiktoken\n",
    "import json\n",
    "import hashlib\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from tqdm.asyncio import tqdm as tqdm_async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5401e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/api_keys/api_keys.json\", \"r\") as f:\n",
    "    api_keys = dict(json.load(f))\n",
    "\n",
    "client = OpenAI(\n",
    "        api_key = api_keys[\"open_ai_key\"],\n",
    "    )\n",
    "    \n",
    "model = \"gpt-4o-mini-2024-07-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842dc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/1_sampled_schema_groups_with_metadata.json\", \"r\") as f:\n",
    "    file = dict(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc286a8",
   "metadata": {},
   "source": [
    "### Initialising the column descriptions\n",
    "\n",
    "Here we format better the structure of the column descriptions and also convert all empty strings and NaN to None for standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491cdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for database in file.keys():\n",
    "\n",
    "    for table in file[database].keys():\n",
    "\n",
    "        for template in file[database][table]['grouped'].keys():\n",
    "\n",
    "            for t in range(len(file[database][table]['grouped'][template])):\n",
    "\n",
    "                column_names = file[database][table]['grouped'][template][t]['details']['columns'].keys()\n",
    "                column_descriptions = file[database][table]['grouped'][template][t]['details']['description']\n",
    "\n",
    "                file[database][table]['grouped'][template][t]['details']['description'] = {\n",
    "                                        col_n: col_d\n",
    "                                            if col_d and isinstance(col_d, str) and len(col_d.split()) > 5 else None\n",
    "                                        for col_n, col_d in zip(column_names, column_descriptions)\n",
    "                                    }\n",
    "\n",
    "        for t in file[database][table]['ungrouped']:\n",
    "\n",
    "            # restructuring file\n",
    "            column_names_types = file[database][table]['ungrouped'][t]['columns']\n",
    "            file[database][table]['ungrouped'][t].pop('columns', None)\n",
    "\n",
    "            sample_row = file[database][table]['ungrouped'][t]['sample_row']\n",
    "            file[database][table]['ungrouped'][t].pop('sample_row', None)\n",
    "\n",
    "            with open(f'/Users/paolocadei/Documents/Masters/Thesis/Spider2/spider2-snow/resource/databases/{database}/{table}/{t}') as f:\n",
    "                table_info = json.load(f)\n",
    "\n",
    "            col_descriptions = table_info['description']\n",
    "\n",
    "            file[database][table]['ungrouped'][t]['details'] = {\n",
    "                'columns': column_names_types,\n",
    "                'description': {\n",
    "                                    col_n: col_d\n",
    "                                        if col_d and isinstance(col_d, str) and len(col_d.split()) > 5 else None\n",
    "                                    for col_n, col_d in zip(column_names_types.keys(), col_descriptions)\n",
    "                                },\n",
    "                'sample_row': sample_row\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9119d",
   "metadata": {},
   "source": [
    "\n",
    "### Inserting the already OpenAI generated descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ebfab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Number of `None` descriptions before update: 71704\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_GREEN_TRIPS_201{variable0}.json'][0]['details']['description']['ehail_fee']:\n",
      "  ‚ûú The ehail_fee represents an additional charge applied to electronic hails for rides booked through a mobile application or dispatch system.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_GREEN_TRIPS_201{variable0}.json'][0]['details']['description']['time_between_service']:\n",
      "  ‚ûú This column represents the time duration in seconds between the end of one taxi trip and the start of the next service trip for a given vehicle, providing insights into operational efficiency and wait times.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_GREEN_TRIPS_201{variable0}.json'][0]['details']['description']['distance_between_service']:\n",
      "  ‚ûú The distance_between_service column records the geographic distance covered during the service period of a trip, measured in miles, and is crucial for understanding trip efficiency and overall travel patterns.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_YELLOW_TRIPS_20{variable0}.json'][0]['details']['description']['dropoff_longitude']:\n",
      "  ‚ûú The dropoff_longitude column represents the geographic longitude coordinate of the drop-off location for each taxi trip in New York City.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_YELLOW_TRIPS_20{variable0}.json'][0]['details']['description']['pickup_latitude']:\n",
      "  ‚ûú The pickup_latitude column captures the geographical latitude coordinate of the location where the taxi trip began, allowing for spatial analysis of pickup points in New York City.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_YELLOW_TRIPS_20{variable0}.json'][0]['details']['description']['dropoff_latitude']:\n",
      "  ‚ûú The dropoff_latitude column records the geographic latitude of the location where a taxi trip ends, providing essential data for analyzing trip destinations and routes in New York City.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_YELLOW_TRIPS_20{variable0}.json'][0]['details']['description']['pickup_longitude']:\n",
      "  ‚ûú This column represents the longitude coordinate of the location where the taxi trip began, providing geographical context for the pickup point.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_FHV_TRIPS_201{variable0}.json'][0]['details']['description']['zone']:\n",
      "  ‚ûú This column represents the specific geographical area or taxi zone where the for-hire vehicle trip was initiated in New York City.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_FHV_TRIPS_201{variable0}.json'][0]['details']['description']['borough']:\n",
      "  ‚ûú This column identifies the borough in which the trip pick-up occurred within New York City, helping to categorize trips by geographic area.\n",
      "\n",
      "[Grouped] Inserted at file['NEW_YORK']['NEW_YORK']['grouped']['TLC_FHV_TRIPS_201{variable0}.json'][0]['details']['description']['service_zone']:\n",
      "  ‚ûú This column indicates the designated service area for the trip, defining the geographic zone within which the for-hire vehicle operates.\n",
      "\n",
      "\n",
      "üîç Number of `None` descriptions before update: 50865\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['group_quarters']:\n",
      "  ‚ûú The total number of individuals residing in group quarters within the geographic area, such as dormitories, nursing homes, military barracks, or correctional facilities, rather than in traditional households.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['black_including_hispanic']:\n",
      "  ‚ûú The number of people in the area who identify as Black or African American, including those who also identify as Hispanic or Latino of any race.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['hispanic_any_race']:\n",
      "  ‚ûú The total population of individuals who identify as Hispanic or Latino, regardless of their race, within the specified geography.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['geo_id']:\n",
      "  ‚ûú A unique geographic identifier code assigned to the specific area covered by the data, used for location referencing and data linking across datasets.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['owner_occupied_housing_units']:\n",
      "  ‚ûú The total number of housing units within the geography that are occupied by their owners rather than rented or vacant.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['do_date']:\n",
      "  ‚ûú The date on which the data or record was officially collected, processed, or reported, serving as a timestamp for the dataset version.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['asian_including_hispanic']:\n",
      "  ‚ûú The number of individuals identifying as Asian, including those who also identify as Hispanic or Latino of any race, within the geographic area.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['unemployed_pop']:\n",
      "  ‚ûú The total number of individuals within the area who are currently unemployed and actively seeking employment according to labor force criteria.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['amerindian_including_hispanic']:\n",
      "  ‚ûú The count of people who identify as American Indian or Alaska Native, including those who also identify as Hispanic or Latino of any race in the specified region.\n",
      "\n",
      "[Grouped] Inserted at file['SDOH']['CENSUS_BUREAU_ACS']['grouped']['STATE_20{variable0}YR.json'][0]['details']['description']['dwellings_2_units']:\n",
      "  ‚ûú The number of residential housing structures within the area that contain exactly two separate living units, such as duplexes or two-family homes.\n",
      "\n",
      "\n",
      "‚úÖ Update Summary:\n",
      "  üîß Total columns updated: 50865\n",
      "  ‚ùó Remaining `None` descriptions: 0\n",
      "  üïµÔ∏è Previously `None`: 50865\n",
      "  üßÆ Final total filled in: 50865\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for responses in ['2_successes.jsonl', '2_successes_multiple.jsonl']:\n",
    "\n",
    "    # Load JSONL responses\n",
    "    with open(responses, 'r') as f:\n",
    "        success_lines = [json.loads(line) for line in f]\n",
    "\n",
    "    # Build lookups\n",
    "    ungrouped_lookup = {}\n",
    "    grouped_lookup = {}\n",
    "\n",
    "    for entry in success_lines:\n",
    "        if entry['status'] != 'success':\n",
    "            continue\n",
    "\n",
    "        if entry['ungrouped_key'] is not None:\n",
    "            ungrouped_lookup[(entry['database'], entry['table'], entry['ungrouped_key'], entry['column'])] = entry['response']['description']\n",
    "        else:\n",
    "            grouped_lookup[(entry['database'], entry['table'], entry['template'], entry['entry_index'], entry['column'])] = entry['response']['description']\n",
    "\n",
    "    # Utility: count None descriptions\n",
    "    def count_none_descriptions(file):\n",
    "        count = 0\n",
    "        for db in file:\n",
    "            for tbl in file[db]:\n",
    "                # Grouped\n",
    "                for tmpl in file[db][tbl].get('grouped', {}):\n",
    "                    for t in file[db][tbl]['grouped'][tmpl]:\n",
    "                        count += sum(1 for desc in t['details']['description'].values() if desc is None)\n",
    "                # Ungrouped\n",
    "                for u_key in file[db][tbl].get('ungrouped', {}):\n",
    "                    descs = file[db][tbl]['ungrouped'][u_key]['details']['description']\n",
    "                    count += sum(1 for desc in descs.values() if desc is None)\n",
    "        return count\n",
    "\n",
    "    # üìä Count before update\n",
    "    before_none = count_none_descriptions(file)\n",
    "    print(f\"\\nüîç Number of `None` descriptions before update: {before_none}\\n\")\n",
    "\n",
    "    # Track printed checks\n",
    "    printed = 0\n",
    "    max_print = 10\n",
    "    updated_count = 0\n",
    "\n",
    "    # üü¢ Grouped updates\n",
    "    for database in file:\n",
    "        for table in file[database]:\n",
    "            for template in file[database][table].get('grouped', {}):\n",
    "                for t_index, item in enumerate(file[database][table]['grouped'][template]):\n",
    "                    for column_name in item['details']['description']:\n",
    "                        if item['details']['description'][column_name] is None:\n",
    "                            key = (database, table, template, t_index, column_name)\n",
    "                            if key in grouped_lookup:\n",
    "                                description = grouped_lookup[key]\n",
    "                                file[database][table]['grouped'][template][t_index]['details']['description'][column_name] = description\n",
    "                                updated_count += 1\n",
    "                                if printed < max_print:\n",
    "                                    print(f\"[Grouped] Inserted at file['{database}']['{table}']['grouped']['{template}'][{t_index}]['details']['description']['{column_name}']:\")\n",
    "                                    print(f\"  ‚ûú {description}\\n\")\n",
    "                                    printed += 1\n",
    "\n",
    "    # üîµ Ungrouped updates\n",
    "    for database in file:\n",
    "        for table in file[database]:\n",
    "            for ungrouped_key, content in file[database][table].get('ungrouped', {}).items():\n",
    "                for column_name in content['details']['description']:\n",
    "                    if content['details']['description'][column_name] is None:\n",
    "                        key = (database, table, ungrouped_key, column_name)\n",
    "                        if key in ungrouped_lookup:\n",
    "                            description = ungrouped_lookup[key]\n",
    "                            file[database][table]['ungrouped'][ungrouped_key]['details']['description'][column_name] = description\n",
    "                            updated_count += 1\n",
    "                            if printed < max_print:\n",
    "                                print(f\"[Ungrouped] Inserted at file['{database}']['{table}']['ungrouped']['{ungrouped_key}']['details']['description']['{column_name}']:\")\n",
    "                                print(f\"  ‚ûú {description}\\n\")\n",
    "                                printed += 1\n",
    "\n",
    "# üìä Count after update\n",
    "after_none = count_none_descriptions(file)\n",
    "\n",
    "# ‚úÖ Summary\n",
    "print(\"\\n‚úÖ Update Summary:\")\n",
    "print(f\"  üîß Total columns updated: {updated_count}\")\n",
    "print(f\"  ‚ùó Remaining `None` descriptions: {after_none}\")\n",
    "print(f\"  üïµÔ∏è Previously `None`: {before_none}\")\n",
    "print(f\"  üßÆ Final total filled in: {before_none - after_none}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39692311",
   "metadata": {},
   "source": [
    "### Generating the prompts for the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0708abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def open_ai_call(client, model, system_prompt, user_prompt, max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def clean_and_parse_result(result):\n",
    "    cleaned = re.sub(r\"^```(?:json)?\\n?\", \"\", result.strip())\n",
    "    cleaned = re.sub(r\"\\n?```$\", \"\", cleaned.strip())\n",
    "\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return ast.literal_eval(cleaned)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    match = re.search(r\"(\\[.*\\]|\\{.*\\})\", cleaned, re.DOTALL)\n",
    "    if match:\n",
    "        snippet = match.group(0)\n",
    "        try:\n",
    "            return json.loads(snippet)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                return ast.literal_eval(snippet)\n",
    "            except Exception as e:\n",
    "                print(\"‚ö†Ô∏è Failed to parse fallback snippet\")\n",
    "                print(snippet)\n",
    "                raise e\n",
    "\n",
    "    raise ValueError(\"‚ö†Ô∏è Could not parse cleaned result.\")\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o-mini-2024-07-18\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def create_column_description_prompt(\n",
    "    target_column_strs,  # now expects a list\n",
    "    table_description,\n",
    "    other_column_descriptions={},\n",
    "    database=None,\n",
    "    table=None,\n",
    "    template=None,\n",
    "    ungrouped_file=None,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    token_limit=500,\n",
    "    allow_sampling_if_too_long=True,\n",
    "    max_columns=25\n",
    "):\n",
    "    def parse_target(target):\n",
    "        if \"::\" not in target:\n",
    "            raise ValueError(f\"Expected format 'column_name::type', got: {target}\")\n",
    "        return target.split(\"::\", 1)\n",
    "\n",
    "    def format_existing_columns(columns_dict):\n",
    "        return [f\"- {col}: {desc}\" for col, desc in columns_dict.items()]\n",
    "\n",
    "    def format_column_names_only(col_names):\n",
    "        return [f\"- {col}\" for col in col_names]\n",
    "\n",
    "    def format_target_columns(targets):\n",
    "        return \"\\n\".join([f\"- {name} ({typ})\" for name, typ in targets])\n",
    "\n",
    "    parsed_targets = [parse_target(target) for target in target_column_strs]\n",
    "\n",
    "    clean_descriptions = {\n",
    "        k: v for k, v in (other_column_descriptions or {}).items()\n",
    "        if v is not None and k not in [name for name, _ in parsed_targets]\n",
    "    }\n",
    "\n",
    "    all_other_column_names = [\n",
    "        k for k in (other_column_descriptions or {}) if k not in [name for name, _ in parsed_targets]\n",
    "    ]\n",
    "\n",
    "    def build_prompt(existing_lines, subset_note):\n",
    "        columns_block = \"\\n\".join(existing_lines) if existing_lines else \"None\"\n",
    "\n",
    "        if template is not None:\n",
    "            intro = (\n",
    "                f\"You are working with columns from the `{template}` table, \"\n",
    "                f\"which belongs to `{table}` in the `{database}` database.\"\n",
    "            )\n",
    "        elif ungrouped_file is not None:\n",
    "            intro = (\n",
    "                f\"You are working with columns from a standalone table `{ungrouped_file}` \"\n",
    "                f\"in `{table}` of the `{database}` database.\"\n",
    "            )\n",
    "        else:\n",
    "            intro = f\"You are working with columns in `{table}` of the `{database}` database.\"\n",
    "\n",
    "        targets_block = format_target_columns(parsed_targets)\n",
    "\n",
    "        return f\"\"\"{intro}\n",
    "\n",
    "- Table Description: {table_description}\n",
    "\n",
    "{subset_note}\n",
    "\n",
    "- Existing Columns:\n",
    "{columns_block}\n",
    "\n",
    "- Target Columns (to describe):\n",
    "{targets_block}\n",
    "\"\"\"\n",
    "\n",
    "    # === CASE 1: No descriptions, only names ===\n",
    "    if not clean_descriptions and all_other_column_names:\n",
    "        fallback_sample = random.sample(all_other_column_names, min(max_columns, len(all_other_column_names)))\n",
    "        fallback_lines = format_column_names_only(fallback_sample)\n",
    "\n",
    "        note = \"Note: Only column names are shown, as no descriptions were available.\"\n",
    "        if len(fallback_sample) < len(all_other_column_names):\n",
    "            note += \" Not all columns are listed.\"\n",
    "\n",
    "        return build_prompt(\n",
    "            fallback_lines,\n",
    "            note\n",
    "        )\n",
    "\n",
    "    # === CASE 2: Try full list ===\n",
    "    col_items = list(clean_descriptions.items())\n",
    "    random.shuffle(col_items)\n",
    "    col_items = col_items[:max_columns]\n",
    "\n",
    "    full_lines = format_existing_columns(dict(col_items))\n",
    "    full_prompt = build_prompt(\n",
    "        full_lines,\n",
    "        \"Note: All columns of the table are listed below.\"\n",
    "    )\n",
    "    if count_tokens(full_prompt, model=model) <= token_limit:\n",
    "        return full_prompt\n",
    "\n",
    "    # === CASE 3: Shrink to fit ===\n",
    "    if not allow_sampling_if_too_long or not col_items:\n",
    "        return build_prompt([], \"Note: No other columns are shown due to token limits.\")\n",
    "\n",
    "    lo, hi = 1, len(col_items)\n",
    "    best_prompt = None\n",
    "\n",
    "    while lo <= hi:\n",
    "        mid = (lo + hi) // 2\n",
    "        sampled_dict = dict(col_items[:mid])\n",
    "        sampled_lines = format_existing_columns(sampled_dict)\n",
    "        prompt = build_prompt(\n",
    "            sampled_lines,\n",
    "            \"Note: Only a subset of the table‚Äôs columns is provided below.\"\n",
    "        )\n",
    "        if count_tokens(prompt, model=model) <= token_limit:\n",
    "            best_prompt = prompt\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            hi = mid - 1\n",
    "\n",
    "    return best_prompt or build_prompt([], \"Note: No other columns are shown due to token limits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1285845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating column prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 134095/134095 [00:00<00:00, 1066982.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Columns without description: 0\n",
      "‚úÖ Columns with description: 134095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_no_desc = 0\n",
    "count_desc = 0\n",
    "\n",
    "grouped_print = 0\n",
    "ungrouped_print = 0\n",
    "\n",
    "# First, count total number of columns to initialize progress bar\n",
    "total_columns = 0\n",
    "for database in file:\n",
    "    for table in file[database]:\n",
    "        for template in file[database][table].get('grouped', {}):\n",
    "            for t in file[database][table]['grouped'][template]:\n",
    "                total_columns += len(t['details']['columns'])\n",
    "        for t in file[database][table].get('ungrouped', {}):\n",
    "            total_columns += len(file[database][table]['ungrouped'][t]['details']['columns'])\n",
    "\n",
    "# Begin processing with progress bar\n",
    "with tqdm(total=total_columns, desc=\"Generating column prompts\") as pbar:\n",
    "    for database in file:\n",
    "        for table in file[database]:\n",
    "\n",
    "            # === GROUPED ===\n",
    "            for template in file[database][table].get('grouped', {}):\n",
    "                for t_index, group_entry in enumerate(file[database][table]['grouped'][template]):\n",
    "                    column_names = group_entry['details']['columns']\n",
    "                    column_descriptions = group_entry['details']['description']\n",
    "                    group_entry['details']['prompts'] = {}\n",
    "\n",
    "                    missing_columns = {\n",
    "                        name: typ for name, typ in column_names.items()\n",
    "                        if column_descriptions[name] is None\n",
    "                    }\n",
    "                    described_columns = set(column_names) - set(missing_columns)\n",
    "\n",
    "                    # Handle described columns\n",
    "                    for col in described_columns:\n",
    "                        group_entry['details']['prompts'][col] = None\n",
    "                        count_desc += 1\n",
    "                        pbar.update(1)\n",
    "\n",
    "                    # Handle missing descriptions in batches of 10\n",
    "                    missing_items = list(missing_columns.items())\n",
    "                    for i in range(0, len(missing_items), 10):\n",
    "                        batch = missing_items[i:i+10]\n",
    "                        target_column_strs = [f\"{name}::{typ}\" for name, typ in batch]\n",
    "\n",
    "                        col_prompt = create_column_description_prompt(\n",
    "                            target_column_strs=target_column_strs,\n",
    "                            table_description=group_entry['description'],\n",
    "                            other_column_descriptions=column_descriptions,\n",
    "                            database=database,\n",
    "                            table=table,\n",
    "                            template=template,\n",
    "                            token_limit=500\n",
    "                        )\n",
    "\n",
    "                        if grouped_print == 0:\n",
    "                            print(col_prompt)\n",
    "                            grouped_print += 1\n",
    "\n",
    "                        for name, _ in batch:\n",
    "                            group_entry['details']['prompts'][name] = col_prompt\n",
    "                            count_no_desc += 1\n",
    "                            pbar.update(1)\n",
    "\n",
    "\n",
    "            # === UNGROUPED ===\n",
    "            for ungrouped_key, ungrouped_entry in file[database][table].get('ungrouped', {}).items():\n",
    "                column_names = ungrouped_entry['details']['columns']\n",
    "                column_descriptions = ungrouped_entry['details']['description']\n",
    "                ungrouped_entry['details']['prompts'] = {}\n",
    "\n",
    "                missing_columns = {\n",
    "                    name: typ for name, typ in column_names.items()\n",
    "                    if column_descriptions[name] is None\n",
    "                }\n",
    "                described_columns = set(column_names) - set(missing_columns)\n",
    "\n",
    "                for col in described_columns:\n",
    "                    ungrouped_entry['details']['prompts'][col] = None\n",
    "                    count_desc += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "                missing_items = list(missing_columns.items())\n",
    "                for i in range(0, len(missing_items), 10):\n",
    "                    batch = missing_items[i:i+10]\n",
    "                    target_column_strs = [f\"{name}::{typ}\" for name, typ in batch]\n",
    "\n",
    "                    col_prompt = create_column_description_prompt(\n",
    "                        target_column_strs=target_column_strs,\n",
    "                        table_description=ungrouped_entry['description'],\n",
    "                        other_column_descriptions=column_descriptions,\n",
    "                        database=database,\n",
    "                        table=table,\n",
    "                        ungrouped_file=ungrouped_key,\n",
    "                        token_limit=500\n",
    "                    )\n",
    "\n",
    "                    if ungrouped_print == 0:\n",
    "                        print(col_prompt)\n",
    "                        ungrouped_print += 1\n",
    "\n",
    "                    for name, _ in batch:\n",
    "                        ungrouped_entry['details']['prompts'][name] = col_prompt\n",
    "                        count_no_desc += 1\n",
    "                        pbar.update(1)\n",
    "\n",
    "\n",
    "print(\"üìù Columns without description:\", count_no_desc)\n",
    "print(\"‚úÖ Columns with description:\", count_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2c273",
   "metadata": {},
   "source": [
    "### Multiple columns at the time code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a12eb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queued 0 prompts to process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 0batch [00:00, ?batch/s]rompt/s]\n",
      "Total Prompts: 0prompt [00:00, ?prompt/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/api_keys/api_keys.json\", \"r\") as f:\n",
    "    api_keys = dict(json.load(f))\n",
    "\n",
    "OPENAI_API_KEY =  api_keys[\"open_ai_key\"]\n",
    "OPENAI_API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "MODEL = \"gpt-4o-mini-2024-07-18\" #'gpt-4.1-mini-2025-04-14'  \"gpt-4o-mini-2024-07-18\"\n",
    "BATCH_SIZE = 15\n",
    "MAX_COUNT = None\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# === LIMITS (75% SAFETY) ===\n",
    "ESTIMATED_TOKENS_PER_CALL = 1000\n",
    "MAX_REQUESTS_PER_MIN = 500\n",
    "MAX_TOKENS_PER_MIN = 120_000\n",
    "MAX_REQUESTS_PER_DAY = 10_000\n",
    "MAX_TOKENS_PER_DAY = 2_000_000\n",
    "\n",
    "# === PROMPT ===\n",
    "system_prompt = \"\"\"You are generating metadata for **multiple columns** in a database table.\n",
    "\n",
    "Your task is to describe what each column contains and how users might refer to it, based on the table's description and other columns.\n",
    "\n",
    "For each column:\n",
    "- Write a concise, clear sentence describing its **meaning and purpose**.\n",
    "- Then provide **10 relevant keywords or phrases** capturing its semantic content (used for semantic search and matching).\n",
    "- Focus on real-world concepts. Avoid redundancy or just repeating the column name.\n",
    "\n",
    "**Output format**:\n",
    "{\n",
    "  \"column_name_1\": {\n",
    "    \"description\": \"...\",\n",
    "    \"keywords\": [\"...\", \"...\"]\n",
    "  },\n",
    "  \"column_name_2\": {\n",
    "    ...\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "# === UTILITIES ===\n",
    "def log_jsonl(path, entry):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "def canonical_hash(meta):\n",
    "    canonical_meta = {\n",
    "        \"database\": meta.get(\"database\"),\n",
    "        \"table\": meta.get(\"table\"),\n",
    "        \"column\": meta.get(\"column\"),\n",
    "        \"template\": meta.get(\"template\", None),\n",
    "        \"ungrouped_key\": meta.get(\"ungrouped_key\", None),\n",
    "        \"entry_index\": meta.get(\"entry_index\", None)\n",
    "    }\n",
    "    return hashlib.md5(json.dumps(canonical_meta, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "def load_hashes_from_jsonl(file_path):\n",
    "    hashes = set()\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line)\n",
    "                hashes.add(canonical_hash(entry))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return hashes\n",
    "\n",
    "async def openai_call(session, prompt, retries=10, wait_time=60):\n",
    "    data = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.post(OPENAI_API_URL, headers=HEADERS, json=data) as resp:\n",
    "                if resp.status == 429:\n",
    "                    raise aiohttp.ClientResponseError(\n",
    "                        status=429, request_info=resp.request_info, history=resp.history, message=\"Rate limit hit\"\n",
    "                    )\n",
    "                resp.raise_for_status()\n",
    "                result = await resp.json()\n",
    "                return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            if e.status == 429 or 500 <= e.status < 600:\n",
    "                if attempt < retries - 1:\n",
    "                    print(f\"[Retrying] Attempt {attempt + 1}/{retries} due to status {e.status}. Waiting {wait_time}s...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"[ERROR] Max retries reached for prompt. Last error: {e}. Waiting 120s before continuing...\")\n",
    "                    await asyncio.sleep(120)\n",
    "                    return await openai_call(session, prompt)  # Recursively retry once more after delay\n",
    "            else:\n",
    "                print(f\"[ERROR] Unhandled response error: {e}\")\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Unexpected exception: {e}\")\n",
    "            raise\n",
    "\n",
    "async def process_prompt(session, col_prompt, columns, meta_template, entry_ref, desc_key, keywords_key):\n",
    "    try:\n",
    "        response = await openai_call(session, col_prompt)\n",
    "        result = json.loads(response)\n",
    "\n",
    "        # 1. Log each column individually (existing behavior)\n",
    "        for column_name in columns:\n",
    "            if column_name in result:\n",
    "                entry_ref[desc_key][column_name] = result[column_name][\"description\"]\n",
    "                entry_ref[keywords_key][column_name] = result[column_name][\"keywords\"]\n",
    "\n",
    "                meta = dict(meta_template)\n",
    "                meta[\"column\"] = column_name\n",
    "\n",
    "                log_jsonl(\"2_successes_multiple.jsonl\", {\n",
    "                    **meta,\n",
    "                    \"status\": \"success\",\n",
    "                    \"prompt\": col_prompt,\n",
    "                    \"response\": result[column_name]\n",
    "                })\n",
    "\n",
    "        # 2. Log full raw response and summary of batch\n",
    "        log_jsonl(\"2_batch_successes.jsonl\", {\n",
    "            \"columns\": columns,\n",
    "            \"meta\": {k: v for k, v in meta_template.items() if k != \"column\"},\n",
    "            \"prompt\": col_prompt,\n",
    "            \"response\": result,\n",
    "            \"status\": \"batch_success\"\n",
    "        })\n",
    "\n",
    "        # 3. Optional: Save full raw response separately for archival\n",
    "        log_jsonl(\"2_raw_responses.jsonl\", {\n",
    "            \"prompt\": col_prompt,\n",
    "            \"response\": result\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        for column_name in columns:\n",
    "            meta = dict(meta_template)\n",
    "            meta[\"column\"] = column_name\n",
    "            log_jsonl(\"failures.jsonl\", {\n",
    "                **meta, \"status\": \"error\", \"prompt\": col_prompt, \"error\": str(e)}\n",
    "            )\n",
    "\n",
    "\n",
    "async def main():\n",
    "    while True:\n",
    "        try:\n",
    "            success_hashes = load_hashes_from_jsonl(\"2_successes_multiple.jsonl\")\n",
    "            tasks = []\n",
    "            count = 0\n",
    "            processed_prompts = set()\n",
    "\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                for database in file:\n",
    "                    for table in file[database]:\n",
    "                        # === GROUPED ===\n",
    "                        for template in file[database][table].get('grouped', {}):\n",
    "                            for t_index, group_entry in enumerate(file[database][table]['grouped'][template]):\n",
    "                                group_entry['details']['keywords'] = {}\n",
    "\n",
    "                                prompt_to_columns = defaultdict(list)\n",
    "                                prompt_to_meta = {}\n",
    "                                prompt_to_entry_ref = {}\n",
    "\n",
    "                                for column_name in group_entry['details']['columns']:\n",
    "                                    prompt = group_entry['details']['prompts'].get(column_name)\n",
    "                                    if prompt is None or group_entry['details']['description'].get(column_name) is not None:\n",
    "                                        continue\n",
    "\n",
    "                                    meta = {\n",
    "                                        \"database\": database,\n",
    "                                        \"table\": table,\n",
    "                                        \"column\": column_name,\n",
    "                                        \"template\": template,\n",
    "                                        \"ungrouped_key\": None,\n",
    "                                        \"entry_index\": t_index\n",
    "                                    }\n",
    "\n",
    "                                    if canonical_hash(meta) in success_hashes:\n",
    "                                        continue\n",
    "\n",
    "                                    prompt_to_columns[prompt].append(column_name)\n",
    "                                    prompt_to_meta[prompt] = meta\n",
    "                                    prompt_to_entry_ref[prompt] = group_entry['details']\n",
    "\n",
    "                                for prompt, columns in prompt_to_columns.items():\n",
    "                                    if prompt in processed_prompts:\n",
    "                                        continue\n",
    "                                    processed_prompts.add(prompt)\n",
    "                                    task = process_prompt(\n",
    "                                        session=session,\n",
    "                                        col_prompt=prompt,\n",
    "                                        columns=columns,\n",
    "                                        meta_template=prompt_to_meta[prompt],\n",
    "                                        entry_ref=prompt_to_entry_ref[prompt],\n",
    "                                        desc_key=\"description\",\n",
    "                                        keywords_key=\"keywords\"\n",
    "                                    )\n",
    "                                    tasks.append(task)\n",
    "                                    count += len(columns)\n",
    "\n",
    "                        # === UNGROUPED ===\n",
    "                        for ungrouped_key, ungrouped_entry in file[database][table].get('ungrouped', {}).items():\n",
    "                            ungrouped_entry['details']['keywords'] = {}\n",
    "\n",
    "                            prompt_to_columns = defaultdict(list)\n",
    "                            prompt_to_meta = {}\n",
    "                            prompt_to_entry_ref = {}\n",
    "\n",
    "                            for column_name in ungrouped_entry['details']['columns']:\n",
    "                                prompt = ungrouped_entry['details']['prompts'].get(column_name)\n",
    "                                if prompt is None or ungrouped_entry['details']['description'].get(column_name) is not None:\n",
    "                                    continue\n",
    "\n",
    "                                meta = {\n",
    "                                    \"database\": database,\n",
    "                                    \"table\": table,\n",
    "                                    \"column\": column_name,\n",
    "                                    \"template\": None,\n",
    "                                    \"ungrouped_key\": ungrouped_key,\n",
    "                                    \"entry_index\": None\n",
    "                                }\n",
    "\n",
    "                                if canonical_hash(meta) in success_hashes:\n",
    "                                    continue\n",
    "\n",
    "                                prompt_to_columns[prompt].append(column_name)\n",
    "                                prompt_to_meta[prompt] = meta\n",
    "                                prompt_to_entry_ref[prompt] = ungrouped_entry['details']\n",
    "\n",
    "                            for prompt, columns in prompt_to_columns.items():\n",
    "                                if prompt in processed_prompts:\n",
    "                                    continue\n",
    "                                processed_prompts.add(prompt)\n",
    "                                task = process_prompt(\n",
    "                                    session=session,\n",
    "                                    col_prompt=prompt,\n",
    "                                    columns=columns,\n",
    "                                    meta_template=prompt_to_meta[prompt],\n",
    "                                    entry_ref=prompt_to_entry_ref[prompt],\n",
    "                                    desc_key=\"description\",\n",
    "                                    keywords_key=\"keywords\"\n",
    "                                )\n",
    "                                tasks.append(task)\n",
    "                                count += len(columns)\n",
    "\n",
    "                print(f\"Queued {len(tasks)} prompts to process...\")\n",
    "\n",
    "                requests_this_minute = 0\n",
    "                tokens_this_minute = 0\n",
    "                requests_today = 0\n",
    "                tokens_today = 0\n",
    "                minute_start = time.time()\n",
    "\n",
    "                total_prompts = len(tasks)\n",
    "                batch_indices = list(range(0, total_prompts, BATCH_SIZE))\n",
    "\n",
    "                with tqdm(total=total_prompts, desc=\"Total Prompts\", unit=\"prompt\") as prompt_bar:\n",
    "                    with tqdm(total=len(batch_indices), desc=\"Batches\", unit=\"batch\") as batch_bar:\n",
    "                        for i in batch_indices:\n",
    "                            now = time.time()\n",
    "                            elapsed = now - minute_start\n",
    "\n",
    "                            if elapsed >= 60:\n",
    "                                requests_this_minute = 0\n",
    "                                tokens_this_minute = 0\n",
    "                                minute_start = now\n",
    "\n",
    "                            batch = tasks[i:i+BATCH_SIZE]\n",
    "\n",
    "                            projected_requests_min = requests_this_minute + len(batch)\n",
    "                            projected_tokens_min = tokens_this_minute + len(batch) * ESTIMATED_TOKENS_PER_CALL\n",
    "                            projected_requests_day = requests_today + len(batch)\n",
    "                            projected_tokens_day = tokens_today + len(batch) * ESTIMATED_TOKENS_PER_CALL\n",
    "\n",
    "                            if projected_requests_day > MAX_REQUESTS_PER_DAY or projected_tokens_day > MAX_TOKENS_PER_DAY:\n",
    "                                print(f\"[STOP] Daily request or token limit reached ({requests_today} requests, {tokens_today} tokens used). Halting.\")\n",
    "                                return\n",
    "\n",
    "                            if projected_requests_min > MAX_REQUESTS_PER_MIN or projected_tokens_min > MAX_TOKENS_PER_MIN:\n",
    "                                wait_time = 60 - elapsed\n",
    "                                print(f\"[WAIT] Sleeping {wait_time:.2f}s to stay within RPM/TPM limits...\")\n",
    "                                await asyncio.sleep(wait_time)\n",
    "                                requests_this_minute = 0\n",
    "                                tokens_this_minute = 0\n",
    "                                minute_start = time.time()\n",
    "\n",
    "                            await asyncio.gather(*batch)\n",
    "\n",
    "                            requests_this_minute += len(batch)\n",
    "                            tokens_this_minute += len(batch) * ESTIMATED_TOKENS_PER_CALL\n",
    "                            requests_today += len(batch)\n",
    "                            tokens_today += len(batch) * ESTIMATED_TOKENS_PER_CALL\n",
    "\n",
    "                            prompt_bar.update(len(batch))\n",
    "                            batch_bar.update(1)\n",
    "\n",
    "            break  # All good, exit loop\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[RETRY LOOP] An error occurred: {e}. Waiting 120s before retrying...\")\n",
    "            await asyncio.sleep(120)\n",
    "\n",
    "# === RUN ASYNC IN JUPYTER ===\n",
    "nest_asyncio.apply()\n",
    "asyncio.get_event_loop().run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b087dd8",
   "metadata": {},
   "source": [
    "### Saving the columns with descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e505d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2_final_structure_with_descriptions.json', 'w') as f:\n",
    "    json.dump(file, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "56ef08a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('2_final_structure_with_descriptions.json', 'r') as f:\n",
    "    file = json.load(f)\n",
    "\n",
    "count_none_descriptions(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568f6f6",
   "metadata": {},
   "source": [
    "### Inserting keywords created before when creating the column descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0c484197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Keywords already present before insertion: 134095\n",
      "üìÇ 2_successes.jsonl:\n",
      "   ‚ûï Keywords added: 0\n",
      "   ‚ö†Ô∏è  Corrupted lines: 0\n",
      "üìÇ 2_successes_multiple.jsonl:\n",
      "   ‚ûï Keywords added: 0\n",
      "   ‚ö†Ô∏è  Corrupted lines: 0\n",
      "üìÇ 2_keyword_results.jsonl:\n",
      "   ‚ûï Keywords added: 0\n",
      "   ‚ö†Ô∏è  Corrupted lines: 0\n",
      "\n",
      "‚úÖ Total keywords added: 0\n",
      "‚ùå Keywords still missing: 0\n",
      "‚ö†Ô∏è Total corrupted JSON lines skipped: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def safe_load_jsonl_lines(filepath):\n",
    "    lines = []\n",
    "    bad_lines = 0\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                lines.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                bad_lines += 1\n",
    "                print(f\"[Warning] Invalid JSON at {filepath}, line {i+1}\")\n",
    "    return lines, bad_lines\n",
    "\n",
    "def inserting_keywords(file, responses):\n",
    "    total_added_keywords = 0\n",
    "    total_missing_keywords = 0\n",
    "    total_bad_lines = 0\n",
    "    total_already_present = 0\n",
    "    per_file_stats = []\n",
    "\n",
    "    # üßÆ Count already existing keywords (non-None)\n",
    "    for database in file:\n",
    "        for table in file[database]:\n",
    "            # Grouped\n",
    "            for template in file[database][table].get('grouped', {}):\n",
    "                for item in file[database][table]['grouped'][template]:\n",
    "                    columns = item['details']['columns']\n",
    "                    keywords = item['details'].get('keywords', {})\n",
    "                    for col in columns:\n",
    "                        if keywords.get(col) is not None:\n",
    "                            total_already_present += 1\n",
    "            # Ungrouped\n",
    "            for content in file[database][table].get('ungrouped', {}).values():\n",
    "                columns = content['details']['columns']\n",
    "                keywords = content['details'].get('keywords', {})\n",
    "                for col in columns:\n",
    "                    if keywords.get(col) is not None:\n",
    "                        total_already_present += 1\n",
    "\n",
    "    # üîÅ Process each response file\n",
    "    for response in responses:\n",
    "        added_keywords = 0\n",
    "        bad_lines = 0\n",
    "\n",
    "        success_lines, bad_count = safe_load_jsonl_lines(response)\n",
    "        bad_lines += bad_count\n",
    "        total_bad_lines += bad_count\n",
    "\n",
    "        # Build lookups\n",
    "        ungrouped_lookup = {}\n",
    "        grouped_lookup = {}\n",
    "\n",
    "        for entry in success_lines:\n",
    "            if entry['status'] != 'success':\n",
    "                continue\n",
    "            if entry['ungrouped_key'] is not None:\n",
    "                ungrouped_lookup[(entry['database'], entry['table'], entry['ungrouped_key'], entry['column'])] = entry['response']['keywords']\n",
    "            else:\n",
    "                grouped_lookup[(entry['database'], entry['table'], entry['template'], entry['entry_index'], entry['column'])] = entry['response']['keywords']\n",
    "\n",
    "        # üü¢ Grouped updates\n",
    "        for database in file:\n",
    "            for table in file[database]:\n",
    "                for template in file[database][table].get('grouped', {}):\n",
    "                    for t_index, item in enumerate(file[database][table]['grouped'][template]):\n",
    "                        if 'keywords' not in item['details']:\n",
    "                            item['details']['keywords'] = {k: None for k in item['details']['columns'].keys()}\n",
    "                        for col in item['details']['columns'].keys():\n",
    "                            current_value = item['details']['keywords'].get(col)\n",
    "                            key = (database, table, template, t_index, col)\n",
    "                            if key in grouped_lookup:\n",
    "                                if current_value is None:\n",
    "                                    added_keywords += 1\n",
    "                                item['details']['keywords'][col] = grouped_lookup[key]\n",
    "\n",
    "        # üîµ Ungrouped updates\n",
    "        for database in file:\n",
    "            for table in file[database]:\n",
    "                for ungrouped_key, content in file[database][table].get('ungrouped', {}).items():\n",
    "                    if 'keywords' not in content['details']:\n",
    "                        content['details']['keywords'] = {k: None for k in content['details']['columns'].keys()}\n",
    "                    for col in content['details']['columns'].keys():\n",
    "                        current_value = content['details']['keywords'].get(col)\n",
    "                        key = (database, table, ungrouped_key, col)\n",
    "                        if key in ungrouped_lookup:\n",
    "                            if current_value is None:\n",
    "                                added_keywords += 1\n",
    "                            content['details']['keywords'][col] = ungrouped_lookup[key]\n",
    "\n",
    "        total_added_keywords += added_keywords\n",
    "        per_file_stats.append((response, added_keywords, bad_lines))\n",
    "\n",
    "    # üîç Count remaining missing keywords\n",
    "    missing_preview = []\n",
    "    for database in file:\n",
    "        for table in file[database]:\n",
    "            # Grouped\n",
    "            for template in file[database][table].get('grouped', {}):\n",
    "                for idx, item in enumerate(file[database][table]['grouped'][template]):\n",
    "                    columns = item['details']['columns']\n",
    "                    keywords = item['details'].get('keywords', {})\n",
    "                    for col in columns:\n",
    "                        kw = keywords.get(col)\n",
    "                        if kw is None:\n",
    "                            total_missing_keywords += 1\n",
    "                            if len(missing_preview) < 5:\n",
    "                                missing_preview.append((\"grouped\", database, table, template, idx, col))\n",
    "            # Ungrouped\n",
    "            for u_key, content in file[database][table].get('ungrouped', {}).items():\n",
    "                columns = content['details']['columns']\n",
    "                keywords = content['details'].get('keywords', {})\n",
    "                for col in columns:\n",
    "                    kw = keywords.get(col)\n",
    "                    if kw is None:\n",
    "                        total_missing_keywords += 1\n",
    "                        if len(missing_preview) < 5:\n",
    "                            missing_preview.append((\"ungrouped\", database, table, u_key, None, col))\n",
    "\n",
    "    # üìä Reporting\n",
    "    print(f\"üìå Keywords already present before insertion: {total_already_present}\")\n",
    "    for filename, added, bad in per_file_stats:\n",
    "        print(f\"üìÇ {filename}:\")\n",
    "        print(f\"   ‚ûï Keywords added: {added}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Corrupted lines: {bad}\")\n",
    "    print(f\"\\n‚úÖ Total keywords added: {total_added_keywords}\")\n",
    "    print(f\"‚ùå Keywords still missing: {total_missing_keywords}\")\n",
    "    print(f\"‚ö†Ô∏è Total corrupted JSON lines skipped: {total_bad_lines}\")\n",
    "\n",
    "    if missing_preview:\n",
    "        print(\"\\nüîç Example missing columns:\")\n",
    "        for kind, db, tbl, tmpl_or_key, idx, col in missing_preview:\n",
    "            if kind == \"grouped\":\n",
    "                print(f\"  ‚û§ Grouped | DB: {db} | Table: {tbl} | Template: {tmpl_or_key} | Row: {idx} | Column: {col}\")\n",
    "            else:\n",
    "                print(f\"  ‚û§ Ungrouped | DB: {db} | Table: {tbl} | Entry: {tmpl_or_key} | Column: {col}\")\n",
    "\n",
    "    return file\n",
    "\n",
    "\n",
    "# Example usage\n",
    "responses = ['2_successes.jsonl', '2_successes_multiple.jsonl', '2_keyword_results.jsonl']\n",
    "file = inserting_keywords(file, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a4f4d",
   "metadata": {},
   "source": [
    "### Column keywords creation for the missing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dc163721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Found 32900 previously processed columns in 2_keyword_results.jsonl\n",
      "üìå Total missing columns to process: 0\n",
      "üì¨ Total prompts to send (batches of ‚â§10): 0\n",
      "üîÑ Sending 0 requests to OpenAI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Fetching keywords: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All requests processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/api_keys/api_keys.json\", \"r\") as f:\n",
    "    api_keys = dict(json.load(f))\n",
    "\n",
    "OPENAI_API_KEY = api_keys[\"open_ai_key\"]\n",
    "OPENAI_API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "MAX_COUNT = None\n",
    "TEMP_FILE = \"2_keyword_results.json.tmp\"\n",
    "FINAL_FILE = \"2_keyword_results.jsonl\"\n",
    "FAILED_FILE = \"2_keyword_failed_requests.jsonl\"\n",
    "REQUEST_SAVE_INTERVAL = 1 # SET TO 10 IF THERE ARE MORE THAN 10\n",
    "MAX_CONCURRENT_REQUESTS = 1\n",
    "\n",
    "ESTIMATED_TOKENS_PER_CALL = 2000\n",
    "MAX_REQUESTS_PER_MIN = 100\n",
    "MAX_TOKENS_PER_MIN = 75000\n",
    "MAX_REQUESTS_PER_DAY = 5000\n",
    "MAX_TOKENS_PER_DAY = 2000000\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are helping build an NL2SQL system that translates natural language questions into SQL queries.\n",
    "\n",
    "Given multiple columns in a database table, your task is to return a dictionary that maps each column name to a list of **10 relevant keywords or phrases** that describe its semantic content.\n",
    "\n",
    "These keywords will be used to match user questions to the correct columns, so focus on how users might naturally refer to each column. Avoid generic terms and do not repeat the column name unless it‚Äôs a common alias.\n",
    "\n",
    "**Output format**:\n",
    "{\n",
    "  \"column_name_1\": [\"keyword 1\", ..., \"keyword 10\"],\n",
    "  \"column_name_2\": [\"keyword 1\", ..., \"keyword 10\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# === Prompt builder ===\n",
    "def build_batch_prompt(database, table, template, column_dict, table_description):\n",
    "    formatted_columns = \"\\n\".join([f\"- {col}: {desc}\" for col, desc in column_dict.items()])\n",
    "    return (\n",
    "        f\"You are working on the table '{template}' in '{table}' from the database '{database}'.\\n\"\n",
    "        f\"The table is described as: {table_description}\\n\\n\"\n",
    "        f\"The following columns need keywords:\\n{formatted_columns}\\n\\n\"\n",
    "        f\"Return a dictionary mapping each column to 10 relevant keywords.\"\n",
    "    )\n",
    "\n",
    "# === OpenAI call with retry and pacing ===\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "requests_this_minute = 0\n",
    "tokens_this_minute = 0\n",
    "requests_today = 0\n",
    "tokens_today = 0\n",
    "minute_start = time.time()\n",
    "\n",
    "async def fetch_keywords(session, prompt, column_ids, retries=5, wait_time=60):\n",
    "    global requests_this_minute, tokens_this_minute, requests_today, tokens_today, minute_start\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        async with semaphore:\n",
    "            now = time.time()\n",
    "            if now - minute_start >= 60:\n",
    "                requests_this_minute = 0\n",
    "                tokens_this_minute = 0\n",
    "                minute_start = now\n",
    "\n",
    "            if (requests_this_minute + 1 > MAX_REQUESTS_PER_MIN or\n",
    "                tokens_this_minute + ESTIMATED_TOKENS_PER_CALL > MAX_TOKENS_PER_MIN):\n",
    "                sleep_time = 60 - (now - minute_start)\n",
    "                print(f\"[WAIT] Sleeping {sleep_time:.1f}s to stay under rate limit...\")\n",
    "                await asyncio.sleep(sleep_time)\n",
    "                requests_this_minute = 0\n",
    "                tokens_this_minute = 0\n",
    "                minute_start = time.time()\n",
    "\n",
    "            try:\n",
    "                async with session.post(OPENAI_API_URL, headers=HEADERS, json=payload) as response:\n",
    "                    if response.status == 429:\n",
    "                        raise aiohttp.ClientResponseError(\n",
    "                            status=429, request_info=response.request_info, history=response.history, message=\"Rate limit hit\"\n",
    "                        )\n",
    "                    response.raise_for_status()\n",
    "                    result = await response.json()\n",
    "                    content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    requests_this_minute += 1\n",
    "                    tokens_this_minute += ESTIMATED_TOKENS_PER_CALL\n",
    "                    requests_today += 1\n",
    "                    tokens_today += ESTIMATED_TOKENS_PER_CALL\n",
    "                    return column_ids, content\n",
    "\n",
    "            except aiohttp.ClientResponseError as e:\n",
    "                if e.status == 429 or 500 <= e.status < 600:\n",
    "                    print(f\"[Retry] {e.status} ‚Äî sleeping {wait_time}s (attempt {attempt + 1})\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"[Error] Non-retryable: {e.status}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"[Exception] {e}\")\n",
    "                break\n",
    "\n",
    "    return column_ids, \"null_response_retry_limit_reached\"\n",
    "\n",
    "# === Save helpers ===\n",
    "def save_tmp(jsonl_lines):\n",
    "    with open(TEMP_FILE, \"w\") as f:\n",
    "        f.write(\"\\n\".join(jsonl_lines) + \"\\n\")\n",
    "    #print(f\"üíæ Temp file {TEMP_FILE} saved.\")\n",
    "\n",
    "def flush_tmp_to_final():\n",
    "    if os.path.exists(TEMP_FILE):\n",
    "        with open(TEMP_FILE, \"r\") as tmp_f:\n",
    "            tmp_data = tmp_f.read()\n",
    "        with open(FINAL_FILE, \"a\") as final_f:\n",
    "            final_f.write(tmp_data)\n",
    "        os.remove(TEMP_FILE)\n",
    "        #print(f\"üì§ Flushed temp file to {FINAL_FILE} and deleted {TEMP_FILE}\")\n",
    "\n",
    "def log_failed(column_ids, raw_content):\n",
    "    with open(FAILED_FILE, \"a\") as f:\n",
    "        f.write(json.dumps({\n",
    "            \"column_ids\": column_ids,\n",
    "            \"response\": raw_content[:300]  # Limit long output\n",
    "        }) + \"\\n\")\n",
    "    print(f\"‚ùå Logged failure for {column_ids} to {FAILED_FILE}\")\n",
    "\n",
    "# === Load processed columns from final file ===\n",
    "def load_processed_column_ids():\n",
    "    if not os.path.exists(FINAL_FILE):\n",
    "        return set()\n",
    "    processed = set()\n",
    "    with open(FINAL_FILE, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                key = (entry[\"database\"], entry[\"table\"], entry[\"column\"])\n",
    "                processed.add(key)\n",
    "            except Exception:\n",
    "                continue\n",
    "    print(f\"üîÅ Found {len(processed)} previously processed columns in {FINAL_FILE}\")\n",
    "    return processed\n",
    "\n",
    "async def process_keywords(file):\n",
    "    processed_columns = load_processed_column_ids()\n",
    "    jsonl_buffer = []\n",
    "    columns_processed = 0\n",
    "    request_batch_buffer = []\n",
    "    total_columns_missing = 0\n",
    "    total_prompts_sent = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "\n",
    "        for database in file:\n",
    "            for table in file[database]:\n",
    "                # GROUPED\n",
    "                for template in file[database][table].get('grouped', {}):\n",
    "                    for entry_index, entry in enumerate(file[database][table]['grouped'][template]):\n",
    "                        table_description = entry.get('description', '')\n",
    "                        columns = entry['details']['columns']\n",
    "                        keywords = entry['details'].get('keywords', {})\n",
    "                        \n",
    "                        missing = {\n",
    "                            column: desc for column, desc in columns.items()\n",
    "                            if keywords.get(column) is None\n",
    "                        }\n",
    "\n",
    "                        total_columns_missing += len(missing)\n",
    "                        for i in range(0, len(missing), 10):\n",
    "                            batch = dict(list(missing.items())[i:i+10])\n",
    "                            if not batch:\n",
    "                                continue\n",
    "                            prompt = build_batch_prompt(database, table, template, batch, table_description)\n",
    "                            column_ids = [(\"grouped\", database, table, template, entry_index, col) for col in batch]\n",
    "                            tasks.append(fetch_keywords(session, prompt, column_ids))\n",
    "                            total_prompts_sent += 1\n",
    "\n",
    "                # UNGROUPED\n",
    "                for ungrouped_key, content in file[database][table].get('ungrouped', {}).items():\n",
    "                    table_description = content.get('description', '')\n",
    "                    columns = content['details']['columns']\n",
    "                    keywords = content['details'].get('keywords', {})\n",
    "                    missing = {\n",
    "                            column: desc for column, desc in columns.items()\n",
    "                            if keywords.get(column) is None\n",
    "                        }\n",
    "                    \n",
    "                    total_columns_missing += len(missing)\n",
    "                    for i in range(0, len(missing), 10):\n",
    "                        batch = dict(list(missing.items())[i:i+10])\n",
    "                        if not batch:\n",
    "                            continue\n",
    "                        prompt = build_batch_prompt(database, table, ungrouped_key, batch, table_description)\n",
    "                        column_ids = [(\"ungrouped\", database, table, ungrouped_key, col) for col in batch]\n",
    "                        tasks.append(fetch_keywords(session, prompt, column_ids))\n",
    "                        total_prompts_sent += 1\n",
    "\n",
    "        # üñ®Ô∏è Print summary before sending requests\n",
    "        print(f\"üìå Total missing columns to process: {total_columns_missing}\")\n",
    "        print(f\"üì¨ Total prompts to send (batches of ‚â§10): {total_prompts_sent}\")\n",
    "        print(f\"üîÑ Sending {len(tasks)} requests to OpenAI...\")\n",
    "\n",
    "        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"üîç Fetching keywords\"):\n",
    "            column_ids, content = await coro\n",
    "            request_batch_buffer.append((column_ids, content))\n",
    "\n",
    "            if len(request_batch_buffer) >= REQUEST_SAVE_INTERVAL:\n",
    "                jsonl_lines = []\n",
    "                for column_ids, content in request_batch_buffer:\n",
    "\n",
    "                    try:\n",
    "                        parsed = json.loads(content)\n",
    "\n",
    "                    except Exception:\n",
    "                        log_failed(column_ids, content)\n",
    "                        continue\n",
    "                    for column_id in column_ids:\n",
    "\n",
    "                        if column_id[0] == \"ungrouped\":\n",
    "                            _, db, tbl, key, col = column_id\n",
    "                            keywords = parsed.get(col)\n",
    "\n",
    "                            if keywords:\n",
    "                                prompt = build_batch_prompt(db, tbl, key, {col: \"\"}, \"\")\n",
    "                                jsonl_lines.append(json.dumps({\n",
    "                                    \"status\": \"success\",\n",
    "                                    \"database\": db,\n",
    "                                    \"table\": tbl,\n",
    "                                    \"template\": None,\n",
    "                                    \"ungrouped_key\": key,\n",
    "                                    \"entry_index\": None,\n",
    "                                    \"column\": col,\n",
    "                                    \"prompt\": prompt,\n",
    "                                    \"response\": {\"keywords\": keywords}\n",
    "                                }))\n",
    "                        else:\n",
    "                            _, db, tbl, template, idx, col = column_id\n",
    "                            keywords = parsed.get(col)\n",
    "                            if keywords:\n",
    "                                prompt = build_batch_prompt(db, tbl, template, {col: \"\"}, \"\")\n",
    "                                jsonl_lines.append(json.dumps({\n",
    "                                    \"status\": \"success\",\n",
    "                                    \"database\": db,\n",
    "                                    \"table\": tbl,\n",
    "                                    \"template\": template,\n",
    "                                    \"ungrouped_key\": None,\n",
    "                                    \"entry_index\": idx,\n",
    "                                    \"column\": col,\n",
    "                                    \"prompt\": prompt,\n",
    "                                    \"response\": {\"keywords\": keywords}\n",
    "                                }))\n",
    "\n",
    "                save_tmp(jsonl_lines)\n",
    "                flush_tmp_to_final()\n",
    "                request_batch_buffer = []\n",
    "\n",
    "        print(\"‚úÖ All requests processed.\")\n",
    "\n",
    "# === Run ===\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(process_keywords(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "39a90efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Keywords already present before insertion: 134095\n",
      "üìÇ 2_keyword_results.jsonl:\n",
      "   ‚ûï Keywords added: 0\n",
      "   ‚ö†Ô∏è  Corrupted lines: 0\n",
      "\n",
      "‚úÖ Total keywords added: 0\n",
      "‚ùå Keywords still missing: 0\n",
      "‚ö†Ô∏è Total corrupted JSON lines skipped: 0\n"
     ]
    }
   ],
   "source": [
    "# Inserting the new values we have generated\n",
    "responses = ['2_keyword_results.jsonl']\n",
    "\n",
    "file = inserting_keywords(file, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9a54761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2_final_structure_all.json', 'w') as f:\n",
    "    json.dump(file, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cf110",
   "metadata": {},
   "source": [
    "### Checking if all columns have the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1ef3d5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columns with keywords: 134095\n",
      "‚ùå Columns missing keywords: 0\n",
      "üìä Total columns: 134095\n"
     ]
    }
   ],
   "source": [
    "def count_keywords_status(file, print_first_missing=True):\n",
    "    count_with_keywords = 0\n",
    "    count_missing_keywords = 0\n",
    "    first_missing_info = None\n",
    "\n",
    "    for db in file:\n",
    "        for tbl in file[db]:\n",
    "            # Grouped\n",
    "            for tmpl in file[db][tbl].get('grouped', {}):\n",
    "                for idx, t in enumerate(file[db][tbl]['grouped'][tmpl]):\n",
    "                    columns = t['details']['columns']\n",
    "                    keywords = t['details'].get('keywords', {})\n",
    "                    for col in columns:\n",
    "                        kw = keywords.get(col)\n",
    "                        if kw is None:\n",
    "                            count_missing_keywords += 1\n",
    "                            if print_first_missing and first_missing_info is None:\n",
    "                                first_missing_info = (\"grouped\", db, tbl, tmpl, idx, col)\n",
    "                        else:\n",
    "                            count_with_keywords += 1\n",
    "\n",
    "            # Ungrouped\n",
    "            for u_key in file[db][tbl].get('ungrouped', {}):\n",
    "                columns = file[db][tbl]['ungrouped'][u_key]['details']['columns']\n",
    "                keywords = file[db][tbl]['ungrouped'][u_key]['details'].get('keywords', {})\n",
    "                for col in columns:\n",
    "                    kw = keywords.get(col)\n",
    "                    if kw is None:\n",
    "                        count_missing_keywords += 1\n",
    "                        if print_first_missing and first_missing_info is None:\n",
    "                            first_missing_info = (\"ungrouped\", db, tbl, u_key, col)\n",
    "                    else:\n",
    "                        count_with_keywords += 1\n",
    "\n",
    "    # Print first missing location if needed\n",
    "    if print_first_missing and first_missing_info:\n",
    "        print(\"üîç First missing keyword found at:\")\n",
    "        if first_missing_info[0] == \"grouped\":\n",
    "            _, db, tbl, tmpl, idx, col = first_missing_info\n",
    "            print(f\"  ‚û§ Grouped | DB: {db} | Table: {tbl} | Template: {tmpl} | Row index: {idx} | Column: {col}\")\n",
    "        else:\n",
    "            _, db, tbl, u_key, col = first_missing_info\n",
    "            print(f\"  ‚û§ Ungrouped | DB: {db} | Table: {tbl} | Entry: {u_key} | Column: {col}\")\n",
    "\n",
    "    return count_with_keywords, count_missing_keywords\n",
    "\n",
    "present, missing = count_keywords_status(file)\n",
    "print(f\"‚úÖ Columns with keywords: {present}\")\n",
    "print(f\"‚ùå Columns missing keywords: {missing}\")\n",
    "print(f\"üìä Total columns: {present + missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae52784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
