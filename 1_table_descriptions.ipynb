{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pprint\n",
    "from itertools import product, islice\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43557de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/api_keys/api_keys.json\", \"r\") as f:\n",
    "    api_keys = dict(json.load(f))\n",
    "\n",
    "client = OpenAI(\n",
    "        api_key = api_keys[\"open_ai_key\"],\n",
    "    )\n",
    "    \n",
    "model = \"gpt-4o-mini-2024-07-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/0_final_preprocessed.json\", \"r\") as f:\n",
    "    schema_groups = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of databases you would like to sample\n",
    "sample_size = len(schema_groups)\n",
    "\n",
    "# Randomly sample keys (database names) from the full schema_groups\n",
    "sampled_keys = random.sample(list(schema_groups.keys()), sample_size)\n",
    "\n",
    "# Build the new sampled dictionary\n",
    "sampled_schema_groups = {k: schema_groups[k] for k in sampled_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db306ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_schema_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b30332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-4o-mini-2024-07-18\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def sample_top_n_columns(columns, descriptions, sample_row, n):\n",
    "    zipped = list(zip(columns.items(), descriptions))\n",
    "\n",
    "    described = [item for item in zipped if item[1]]\n",
    "    others = [item for item in zipped if not item[1]]\n",
    "\n",
    "    sampled = described[:n]\n",
    "    if len(sampled) < n:\n",
    "        sampled += others[: n - len(sampled)]\n",
    "\n",
    "    if not sampled and len(zipped) > 0:\n",
    "        sampled = zipped[:n]\n",
    "\n",
    "    return sampled\n",
    "\n",
    "def create_prompt(\n",
    "    database,\n",
    "    database2,\n",
    "    table_template,\n",
    "    columns,\n",
    "    descriptions,\n",
    "    sample_row,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    token_limit=20000,\n",
    "    allow_sampling_if_too_long=True,\n",
    "    is_grouped=True,\n",
    "    combinations=None,\n",
    "    variable_order=None,\n",
    "):\n",
    "    # Normalize sample_row to always be a list of dicts\n",
    "    if isinstance(sample_row, dict):\n",
    "        sample_row = [sample_row]\n",
    "    elif not isinstance(sample_row, list):\n",
    "        sample_row = []\n",
    "    sample_row = [row for row in sample_row if isinstance(row, dict)]\n",
    "\n",
    "    def get_column_sample(col_name):\n",
    "        # Return up to 2 sample values from sample_row\n",
    "        if not sample_row:\n",
    "            return [\"N/A\"]\n",
    "        return [row.get(col_name, \"N/A\") for row in sample_row[:2]]\n",
    "\n",
    "    def build_prompt(selected_columns, all_columns_used=False):\n",
    "        if is_grouped:\n",
    "            prompt = (\n",
    "                f\"**Database**: {database}.{database2}\\n\"\n",
    "                f\"**Table template**: {table_template}\\n\"\n",
    "                f\"Variables: {', '.join(variable_order)}\\n\"\n",
    "                f\"Sample values: {combinations[:5]}\\n\"\n",
    "                \"Each value generates a specific table instance (e.g. 'TLC_YELLOW_TRIPS_2016.json').\\n\\n\"\n",
    "            )\n",
    "        else:\n",
    "            prompt = (\n",
    "                f\"**Database**: {database}.{database2}\\n\"\n",
    "                f\"**Table name**: {table_template}\\n\\n\"\n",
    "            )\n",
    "\n",
    "        column_text = \"**Columns**:\\n\"\n",
    "        for (col_name, col_type), desc in selected_columns:\n",
    "            samples = get_column_sample(col_name)\n",
    "            if all_columns_used and len(samples) > 1:\n",
    "                sample_str = f\"[{samples[0]}, {samples[1]}]\"\n",
    "            else:\n",
    "                sample_str = samples[0]\n",
    "            desc_text = desc if desc else \"\"\n",
    "            column_text += f\"- {col_name} (type: {col_type}, sample: {sample_str}) – {desc_text}\\n\"\n",
    "\n",
    "        return prompt + column_text\n",
    "\n",
    "    total_columns = len(columns)\n",
    "\n",
    "    if not allow_sampling_if_too_long:\n",
    "        selected_columns = sample_top_n_columns(columns, descriptions, sample_row, total_columns)\n",
    "        prompt = build_prompt(selected_columns, all_columns_used=True)\n",
    "        token_count = count_tokens(prompt, model=model)\n",
    "        return prompt\n",
    "\n",
    "    for n_cols in range(total_columns, -1, -1):\n",
    "        selected_columns = sample_top_n_columns(columns, descriptions, sample_row, n_cols)\n",
    "        all_used = (n_cols == total_columns)\n",
    "        prompt = build_prompt(selected_columns, all_columns_used=all_used)\n",
    "        token_count = count_tokens(prompt, model=model)\n",
    "\n",
    "        if token_count <= token_limit:\n",
    "            return prompt\n",
    "\n",
    "    print(f\"⚠️ Prompt exceeds limit even with 0 columns. Returning fallback.\")\n",
    "    return build_prompt([], all_columns_used=False)\n",
    "\n",
    "\n",
    "\n",
    "def clean_and_parse_result(result):\n",
    "    \"\"\"\n",
    "    Cleans LLM response and returns a parsed Python object.\n",
    "    Handles triple backticks, extra text, or malformed JSON by trying both json and ast parsing.\n",
    "    Supports top-level dicts and lists.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove triple backticks or ```json\n",
    "    cleaned = re.sub(r\"^```(?:json)?\\n?\", \"\", result.strip())\n",
    "    cleaned = re.sub(r\"\\n?```$\", \"\", cleaned.strip())\n",
    "\n",
    "    # Step 2: Try parsing the entire cleaned text as JSON\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Step 3: Try parsing as a Python literal\n",
    "    try:\n",
    "        return ast.literal_eval(cleaned)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Step 4: Try to extract a list or dict substring (fallback)\n",
    "    match = re.search(r\"(\\[.*\\]|\\{.*\\})\", cleaned, re.DOTALL)\n",
    "    if match:\n",
    "        snippet = match.group(0)\n",
    "        try:\n",
    "            return json.loads(snippet)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                return ast.literal_eval(snippet)\n",
    "            except Exception as e:\n",
    "                print(\"⚠️ Failed to parse fallback snippet\")\n",
    "                print(snippet)\n",
    "                raise e\n",
    "\n",
    "    print(\"⚠️ Could not parse result at all.\")\n",
    "    raise ValueError(\"Could not parse cleaned result.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca0414",
   "metadata": {},
   "source": [
    "## Creating the prompts for table description generation\n",
    "\n",
    "Initial structure of schema group:\n",
    "\n",
    "- database (1st folder)\n",
    "    - database2 (2nd folder)\n",
    "        - grouped\n",
    "            - table template (.json)\n",
    "                - **lists** of dictionaries that share the same template with keys\n",
    "                    - variables <- dictionary with key = variable0, variable1, etc...: value = list of values that variables can take\n",
    "                    - combinations <- possible combinations the variables can take in the template\n",
    "                    - variable_order <- order in which the variables should be inserted in the template\n",
    "                    - details <- dictionary\n",
    "                        - columns <- column_name: column_type as string\n",
    "                        - description <- column_description\n",
    "                        - sample_row <- a sample of one row from it if available\n",
    "        - ungrouped\n",
    "            - table_name (.json)\n",
    "                - columns <- column_name: column_type as string\n",
    "                - description <- column_description\n",
    "                - sample_row <- a sample of one row from it if available\n",
    "\n",
    "After this step:\n",
    "\n",
    "- database (1st folder)\n",
    "    - database2 (2nd folder)\n",
    "        - grouped\n",
    "            - table template (.json)\n",
    "                - **lists** of dictionaries that share the same template with keys\n",
    "                    - variables <- dictionary with key = variable0, variable1, etc...: value = list of values that variables can take\n",
    "                    - combinations <- possible combinations the variables can take in the template\n",
    "                    - variable_order <- order in which the variables should be inserted in the template\n",
    "                    - details <- dictionary\n",
    "                        - columns <- column_name: column_type as string\n",
    "                        - description <- column_description\n",
    "                        - sample_row <- a sample of one row from it if available\n",
    "                    - prompt <- prompt used to make a request of the LLM for the description of the table\n",
    "                    - description <- description outputted by the LLM for the table\n",
    "        - ungrouped\n",
    "            - table_name (.json)\n",
    "                - columns <- column_name: column_type as string\n",
    "                - description <- column_description\n",
    "                - sample_row <- a sample of one row from it if available\n",
    "                - prompt <- prompt used to make a request of the LLM for the description of the table\n",
    "                - description <- description outputted by the LLM for the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "# database level\n",
    "for database in tqdm(sampled_schema_groups, desc=\"Databases\"):\n",
    "\n",
    "    # this is the second folder, constituting the smaller database\n",
    "    for database2 in sampled_schema_groups[database]:\n",
    "\n",
    "\n",
    "        # here we will be taking into account the GROUPED SCHEMAS\n",
    "\n",
    "        for tables in sampled_schema_groups[database][database2]['grouped']:\n",
    "\n",
    "            for t in range(len(sampled_schema_groups[database][database2]['grouped'][tables])):\n",
    "\n",
    "                combinations = sampled_schema_groups[database][database2]['grouped'][tables][t]['combinations']\n",
    "                variable_order = sampled_schema_groups[database][database2]['grouped'][tables][t]['variable_order']\n",
    "\n",
    "                columns = sampled_schema_groups[database][database2]['grouped'][tables][t]['details']['columns']\n",
    "                col_desc = sampled_schema_groups[database][database2]['grouped'][tables][t]['details']['description']\n",
    "                sample_row = sampled_schema_groups[database][database2]['grouped'][tables][t]['details']['sample_row']\n",
    "\n",
    "\n",
    "                prompt = create_prompt(\n",
    "                            database = database,\n",
    "                            database2 = database2,\n",
    "                            table_template = tables,\n",
    "                            columns = columns,\n",
    "                            descriptions = col_desc,\n",
    "                            sample_row = sample_row,\n",
    "                            allow_sampling_if_too_long=True,\n",
    "                            is_grouped=True,\n",
    "                            combinations=combinations,\n",
    "                            variable_order=variable_order\n",
    "                        )\n",
    "\n",
    "                sampled_schema_groups[database][database2]['grouped'][tables][t]['prompt'] = prompt\n",
    "\n",
    "                sampled_schema_groups[database][database2]['grouped'][tables][t]['system_role'] = (\n",
    "                            \"You are generating metadata for a **grouped database table template**.\\n\"\n",
    "                            \"Each table in the group follows the same schema but varies by year, region, or other dimensions.\\n\"\n",
    "                            \"Your task is to describe what kind of data is captured across the group — not for a single instance.\\n\\n\"\n",
    "                            \"Instructions:\\n\"\n",
    "                            \"- Write a concise 1–2 sentence **description** of what this group of tables contains, based on the database name, table name, column names, types, description and sample row.\\n\"\n",
    "                            \"- Then provide at least 10 **keywords or phrases** users might use to search for tables like this.\\n\"\n",
    "                            \"- Focus on concepts, entities, or analysis that the data enables. Avoid file names or exact column names.\\n\\n\"\n",
    "                            \"**Output format**:\\n\"\n",
    "                            \"{\\n\"\n",
    "                            '  \"description\": \"...\",\\n'\n",
    "                            '  \"keywords\": [\"...\", \"...\", ...]\\n'\n",
    "                            \"}\"\n",
    "                        )\n",
    "            \n",
    "        for table in sampled_schema_groups[database][database2]['ungrouped']:\n",
    "\n",
    "                columns = sampled_schema_groups[database][database2]['ungrouped'][table]['columns']\n",
    "                col_desc = sampled_schema_groups[database][database2]['ungrouped'][table]['description']\n",
    "                sample_row = sampled_schema_groups[database][database2]['ungrouped'][table]['sample_row']\n",
    "                \n",
    "                prompt = create_prompt(\n",
    "                            database = database,\n",
    "                            database2 = database2,\n",
    "                            table_template = table,\n",
    "                            columns = columns,\n",
    "                            descriptions = col_desc,\n",
    "                            sample_row = sample_row,\n",
    "                            allow_sampling_if_too_long=True,\n",
    "                            is_grouped=False\n",
    "                        )\n",
    "\n",
    "                sampled_schema_groups[database][database2]['ungrouped'][table]['prompt'] = prompt\n",
    "\n",
    "                sampled_schema_groups[database][database2]['ungrouped'][table]['system_role'] = (\n",
    "                            \"You are generating metadata for a **single database table**.\\n\"\n",
    "                            \"This table is not part of a larger group or template.\\n\"\n",
    "                            \"Your task is to describe exactly what this table contains and how users might refer to it, based on the database name, table name, column names, types, descriptions and sample row\\n\\n\"\n",
    "                            \"Instructions:\\n\"\n",
    "                            \"- Write a concise 1–2 sentence **description** of what this table contains.\\n\"\n",
    "                            \"- Then provide at least 10 **keywords or phrases** users might use to search for this table.\\n\"\n",
    "                            \"- Focus on meaningful terms and real-world concepts. Avoid repeating column names unless standard terminology.\\n\\n\"\n",
    "                            \"**Output format**:\\n\"\n",
    "                            \"{\\n\"\n",
    "                            '  \"description\": \"...\",\\n'\n",
    "                            '  \"keywords\": [\"...\", \"...\", ...]\\n'\n",
    "                            \"}\"\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43a791",
   "metadata": {},
   "source": [
    "## Getting the batch of prompts to send to openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jsonl_batch_file(\n",
    "    sampled_schema_groups,\n",
    "    output_path=\"1_batch_requests.jsonl\",\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    max_tokens=1000\n",
    "):\n",
    "    jsonl_lines = []\n",
    "\n",
    "    for database in tqdm(sampled_schema_groups, desc=\"Generating JSONL\"):\n",
    "        for database2 in sampled_schema_groups[database]:\n",
    "\n",
    "            # GROUPED TABLES\n",
    "            for table in sampled_schema_groups[database][database2].get(\"grouped\", {}):\n",
    "                for idx, entry in enumerate(sampled_schema_groups[database][database2][\"grouped\"][table]):\n",
    "                    prompt = entry.get(\"prompt\")\n",
    "                    system_prompt = entry.get(\"system_role\")\n",
    "                    if prompt:\n",
    "                        custom_id = f\"{database}::{database2}::{table}::grouped::{idx}\"\n",
    "                        jsonl_lines.append({\n",
    "                            \"custom_id\": custom_id,\n",
    "                            \"method\": \"POST\",\n",
    "                            \"url\": \"/v1/chat/completions\",\n",
    "                            \"body\": {\n",
    "                                \"model\": model,\n",
    "                                \"messages\": [\n",
    "                                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                    {\"role\": \"user\", \"content\": prompt}\n",
    "                                ],\n",
    "                                \"max_tokens\": max_tokens\n",
    "                            }\n",
    "                        })\n",
    "\n",
    "            # UNGROUPED TABLES\n",
    "            for table in sampled_schema_groups[database][database2].get(\"ungrouped\", {}):\n",
    "                entry = sampled_schema_groups[database][database2][\"ungrouped\"][table]\n",
    "                prompt = entry.get(\"prompt\")\n",
    "                system_prompt = entry.get(\"system_role\")\n",
    "                if prompt:\n",
    "                    custom_id = f\"{database}::{database2}::ungrouped::{table}\"\n",
    "                    jsonl_lines.append({\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"method\": \"POST\",\n",
    "                        \"url\": \"/v1/chat/completions\",\n",
    "                        \"body\": {\n",
    "                            \"model\": model,\n",
    "                            \"messages\": [\n",
    "                                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                {\"role\": \"user\", \"content\": prompt}\n",
    "                            ],\n",
    "                            \"max_tokens\": max_tokens\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "    # Save to .jsonl\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for line in jsonl_lines:\n",
    "            f.write(json.dumps(line) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Batch file saved to: {output_path} with {len(jsonl_lines)} requests.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb15c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_jsonl_batch_file(\n",
    "    sampled_schema_groups,\n",
    "    output_path=\"1_openai_batch_prompts.jsonl\",\n",
    "    model=\"gpt-4o-mini-2024-07-18\",  # or another supported chat model\n",
    "    max_tokens=450\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de651c0d",
   "metadata": {},
   "source": [
    "## Creating multiple file batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_jsonl_by_token_limit(input_path, token_limit, model=\"gpt-4o-mini-2024-07-18\"):\n",
    "    tokenizer = tiktoken.encoding_for_model(model)\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    token_counts = []  # to store total tokens per batch\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            messages = item[\"body\"][\"messages\"]\n",
    "            full_prompt = \" \".join(m[\"content\"] for m in messages)\n",
    "            token_count = len(tokenizer.encode(full_prompt))\n",
    "\n",
    "            if current_tokens + token_count > token_limit:\n",
    "                batches.append(current_batch)\n",
    "                token_counts.append(current_tokens)\n",
    "                current_batch = []\n",
    "                current_tokens = 0\n",
    "\n",
    "            current_batch.append(item)\n",
    "            current_tokens += token_count\n",
    "\n",
    "        if current_batch:\n",
    "            batches.append(current_batch)\n",
    "            token_counts.append(current_tokens)\n",
    "\n",
    "    # Print summary\n",
    "    for i, (batch, tokens) in enumerate(zip(batches, token_counts), 1):\n",
    "        print(f\"📦 Batch {i}: {len(batch)} requests, {tokens} tokens\")\n",
    "\n",
    "    return batches\n",
    "\n",
    "def save_batches_to_files(batches, output_dir=\"openai_batches\", prefix=\"batch\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    paths = []\n",
    "\n",
    "    for i, batch in enumerate(batches):\n",
    "        path = os.path.join(output_dir, f\"{prefix}_{i+1}.jsonl\")\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in batch:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "        paths.append(path)\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "# Split and save\n",
    "batches = split_jsonl_by_token_limit(\n",
    "    input_path=\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/1_openai_batch_prompts.jsonl\",\n",
    "    token_limit=500000\n",
    ")\n",
    "\n",
    "jsonl_paths = save_batches_to_files(batches, output_dir=\"openai_batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3706a6",
   "metadata": {},
   "source": [
    "## Sending batches to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def submit_batches_sequentially(\n",
    "    client,\n",
    "    jsonl_paths,\n",
    "    output_dir=\"1_batch_outputs\",\n",
    "    log_file=\"1_completed_batches.txt\",\n",
    "    failed_log_file=\"1_failed_batches.jsonl\",\n",
    "    poll_interval=100\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for path in jsonl_paths:\n",
    "        print(f\"🚀 Submitting: {path}\")\n",
    "        try:\n",
    "            # Upload file\n",
    "            file_obj = client.files.create(file=open(path, \"rb\"), purpose=\"batch\")\n",
    "\n",
    "            # Submit batch\n",
    "            batch = client.batches.create(\n",
    "                input_file_id=file_obj.id,\n",
    "                endpoint=\"/v1/chat/completions\",\n",
    "                completion_window=\"24h\",\n",
    "                metadata={\"description\": f\"Auto-submitted from {path}\"}\n",
    "            )\n",
    "\n",
    "            print(f\"🆔 Submitted batch ID: {batch.id}\")\n",
    "\n",
    "            # Wait for completion\n",
    "            while True:\n",
    "                status = client.batches.retrieve(batch.id).status\n",
    "                print(f\"⏳ Waiting... Current status of {batch.id}: {status}\")\n",
    "                if status == \"completed\":\n",
    "                    print(f\"✅ Batch {batch.id} completed.\")\n",
    "                    break\n",
    "                elif status in [\"failed\", \"cancelled\"]:\n",
    "                    print(f\"❌ Batch {batch.id} ended with status: {status}\")\n",
    "                    raise RuntimeError(f\"Batch {batch.id} failed with status: {status}\")\n",
    "                time.sleep(poll_interval)\n",
    "\n",
    "            # Fetch output and save to JSONL\n",
    "            batch = client.batches.retrieve(batch.id)\n",
    "            response_id = dict(batch).get(\"output_file_id\")\n",
    "\n",
    "            if response_id:\n",
    "                file_response = client.files.content(response_id)\n",
    "                output_path = os.path.join(output_dir, f\"{batch.id}_output.jsonl\")\n",
    "\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "                    for line in file_response.iter_lines():\n",
    "                        if line:\n",
    "                            try:\n",
    "                                line_json = json.loads(line)\n",
    "                                content_str = line_json[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "                                assistant_content = json.loads(content_str)\n",
    "                                formatted = {\n",
    "                                    \"custom_id\": line_json.get(\"custom_id\"),\n",
    "                                    \"description\": assistant_content.get(\"description\"),\n",
    "                                    \"keywords\": assistant_content.get(\"keywords\")\n",
    "                                }\n",
    "                                out_f.write(json.dumps(formatted) + \"\\n\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"❗ Error parsing line: {e}\")\n",
    "\n",
    "                print(f\"📝 Saved output to: {output_path}\")\n",
    "\n",
    "            # Log batch ID\n",
    "            with open(log_file, \"a\") as log:\n",
    "                log.write(batch.id + \"\\n\")\n",
    "                print(f\"🗂️ Logged batch ID: {batch.id} to {log_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_entry = {\n",
    "                \"file_path\": path,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error_message\": str(e)\n",
    "            }\n",
    "            print(f\"❌ Failed to process batch from {path}: {e}\")\n",
    "            with open(failed_log_file, \"a\") as f:\n",
    "                f.write(json.dumps(error_entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonl_paths_from_folder(folder_path):\n",
    "    return [\n",
    "        os.path.join(folder_path, fname)\n",
    "        for fname in sorted(os.listdir(folder_path))\n",
    "        if fname.endswith(\".jsonl\")\n",
    "    ]\n",
    "\n",
    "# uncomment this for all the batches\n",
    "'''# Path to the folder with batches\n",
    "folder_path = \"/Users/paolocadei/Documents/Masters/Thesis/Spider2/openai_batches\"\n",
    "\n",
    "# Collect all batch paths\n",
    "file_paths = get_jsonl_paths_from_folder(folder_path)\n",
    "'''\n",
    "\n",
    "# uncomment this for the failed batches\n",
    "file_paths = []\n",
    "\n",
    "with open(\"1_failed_batches.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                file_path = entry.get(\"file_path\")\n",
    "                if file_path:\n",
    "                    file_paths.append(file_path)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Skipping invalid line: {e}\")\n",
    "\n",
    "print(file_paths)  # or return, or use however you want\n",
    "\n",
    "# Submit all batches sequentially\n",
    "submit_batches_sequentially(\n",
    "    client,\n",
    "    jsonl_paths=file_paths,\n",
    "    output_dir=\"1_batch_outputs\",\n",
    "    log_file=\"1_completed_batches.txt\",\n",
    "    failed_log_file=\"1_failed_batches.jsonl\",\n",
    "    poll_interval=100  # every ~1.5 minutes\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d02a74",
   "metadata": {},
   "source": [
    "## Submit batches in singular calls for the ones that failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PROMPTS = None  # 👈 Set to None to run all\n",
    "SUCCESS_LOG = \"1_manually_recovered_prompts.jsonl\"\n",
    "FAILURE_LOG = \"1_manual_failures.jsonl\"\n",
    "\n",
    "def load_prompts_from_failed_batch_file(file_path):\n",
    "    prompts = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                prompts.append(json.loads(line))\n",
    "    return prompts\n",
    "\n",
    "def open_ai_call(client, model, system_prompt, user_prompt, max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def clean_and_parse_result(result):\n",
    "    cleaned = re.sub(r\"^```(?:json)?\\n?\", \"\", result.strip())\n",
    "    cleaned = re.sub(r\"\\n?```$\", \"\", cleaned.strip())\n",
    "\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return ast.literal_eval(cleaned)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    match = re.search(r\"(\\[.*\\]|\\{.*\\})\", cleaned, re.DOTALL)\n",
    "    if match:\n",
    "        snippet = match.group(0)\n",
    "        try:\n",
    "            return json.loads(snippet)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                return ast.literal_eval(snippet)\n",
    "            except Exception as e:\n",
    "                print(\"⚠️ Failed to parse fallback snippet\")\n",
    "                print(snippet)\n",
    "                raise e\n",
    "\n",
    "    raise ValueError(\"⚠️ Could not parse cleaned result.\")\n",
    "\n",
    "# Load already successful custom_ids\n",
    "processed_custom_ids = set()\n",
    "if os.path.exists(SUCCESS_LOG):\n",
    "    with open(SUCCESS_LOG, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    processed_custom_ids.add(entry.get(\"custom_id\"))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "# Load all prompts from all failed batch files\n",
    "failed_file_paths = []\n",
    "with open(\"1_failed_batches.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                file_path = entry.get(\"file_path\")\n",
    "                if file_path:\n",
    "                    failed_file_paths.append(file_path)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Skipping invalid line: {e}\")\n",
    "\n",
    "# Load all prompts from all failed batch files\n",
    "failed_file_paths = []\n",
    "with open(\"1_failed_batches.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                file_path = entry.get(\"file_path\")\n",
    "                if file_path:\n",
    "                    failed_file_paths.append(file_path)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Skipping invalid line: {e}\")\n",
    "\n",
    "print(f\"\\n📦 Batch files to process ({len(failed_file_paths)} total):\")\n",
    "for path in failed_file_paths:\n",
    "    print(f\"  - {path}\")\n",
    "\n",
    "\n",
    "all_prompts = []\n",
    "for file_path in failed_file_paths:\n",
    "    try:\n",
    "        prompts = load_prompts_from_failed_batch_file(file_path)\n",
    "        all_prompts.extend(prompts)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Skipped file {file_path}: {e}\")\n",
    "\n",
    "print(f\"🧩 {len(all_prompts)} total prompts loaded from {len(failed_file_paths)} batch files.\")\n",
    "print(f\"🔄 Resuming from {len(processed_custom_ids)} already processed prompts.\")\n",
    "\n",
    "calls_made = 0\n",
    "with open(SUCCESS_LOG, \"a\", encoding=\"utf-8\") as success_f, open(FAILURE_LOG, \"a\", encoding=\"utf-8\") as fail_f:\n",
    "    for request in tqdm(all_prompts, desc=\"🚀 Processing prompts\"):\n",
    "        if MAX_PROMPTS is not None and calls_made >= MAX_PROMPTS:\n",
    "            print(\"✅ Reached MAX_PROMPTS limit.\")\n",
    "            break\n",
    "\n",
    "        custom_id = request.get(\"custom_id\")\n",
    "        if custom_id in processed_custom_ids:\n",
    "            continue\n",
    "\n",
    "        body = request.get(\"body\", {})\n",
    "        model = body.get(\"model\")\n",
    "        max_tokens = body.get(\"max_tokens\", 300)\n",
    "        messages = body.get(\"messages\", [])\n",
    "\n",
    "        system_prompt = next((m[\"content\"] for m in messages if m[\"role\"] == \"system\"), \"\")\n",
    "        user_prompt = next((m[\"content\"] for m in messages if m[\"role\"] == \"user\"), \"\")\n",
    "\n",
    "        try:\n",
    "            raw_output = open_ai_call(client, model, system_prompt, user_prompt, max_tokens=max_tokens)\n",
    "            parsed = clean_and_parse_result(raw_output)\n",
    "\n",
    "            formatted = {\n",
    "                \"custom_id\": custom_id,\n",
    "                \"description\": parsed.get(\"description\"),\n",
    "                \"keywords\": parsed.get(\"keywords\")\n",
    "            }\n",
    "\n",
    "            success_f.write(json.dumps(formatted) + \"\\n\")\n",
    "            success_f.flush()\n",
    "            calls_made += 1\n",
    "            processed_custom_ids.add(custom_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed for {custom_id}: {e}\")\n",
    "            fail_f.write(json.dumps({\"custom_id\": custom_id, \"error\": str(e)}) + \"\\n\")\n",
    "            fail_f.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33136be",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e720af",
   "metadata": {},
   "source": [
    "### Taking all the information from the batched requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_batches_from_ids(batch_id_file=\"1_completed_batches.txt\", output_path=\"1_merged_openai_output.jsonl\"):\n",
    "    merged_lines = []\n",
    "\n",
    "    # Read all batch IDs from file\n",
    "    with open(batch_id_file, \"r\") as f:\n",
    "        batch_ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(f\"🔍 Found {len(batch_ids)} batch IDs to process\")\n",
    "\n",
    "    for batch_id in batch_ids:\n",
    "        try:\n",
    "            print(f\"📦 Retrieving batch: {batch_id}\")\n",
    "            batch = client.batches.retrieve(batch_id)\n",
    "            response_id = dict(batch).get(\"output_file_id\")\n",
    "\n",
    "            if not response_id:\n",
    "                print(f\"⚠️ No response file for batch {batch_id}\")\n",
    "                continue\n",
    "\n",
    "            file_response = client.files.content(response_id)\n",
    "\n",
    "            for line in file_response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        line_json = json.loads(line)\n",
    "                        content_str = line_json[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "                        assistant_content = json.loads(content_str)\n",
    "                        formatted = {\n",
    "                            \"custom_id\": line_json.get(\"custom_id\"),\n",
    "                            \"description\": assistant_content.get(\"description\"),\n",
    "                            \"keywords\": assistant_content.get(\"keywords\")\n",
    "                        }\n",
    "                        merged_lines.append(formatted)\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Failed to parse line in batch {batch_id}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to process batch {batch_id}: {e}\")\n",
    "\n",
    "    # Save merged results\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in merged_lines:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Merged {len(merged_lines)} results into: {output_path}\")\n",
    "\n",
    "merge_batches_from_ids(\n",
    "    batch_id_file=\"1_completed_batches.txt\",\n",
    "    output_path=\"1_merged_openai_output.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8dcd51",
   "metadata": {},
   "source": [
    "### Injecting the json of schema information with the descriptions\n",
    "\n",
    "This includes both the batched descriptions and the singular API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c99503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def inject_metadata_from_file(file_path, sampled_schema_groups):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            entry = json.loads(line)\n",
    "            custom_id = entry.get(\"custom_id\")\n",
    "            description = entry.get(\"description\")\n",
    "            keywords = entry.get(\"keywords\")\n",
    "\n",
    "            if not custom_id or not description or not keywords:\n",
    "                continue\n",
    "\n",
    "            parts = custom_id.split(\"::\")\n",
    "\n",
    "            try:\n",
    "                if \"ungrouped\" in parts:\n",
    "                    # Format: database::database2::ungrouped::table\n",
    "                    database, database2, _, table = parts\n",
    "                    target = sampled_schema_groups[database][database2][\"ungrouped\"][table]\n",
    "                elif \"grouped\" in parts:\n",
    "                    # Format: database::database2::table::grouped::index\n",
    "                    database, database2, table, _, index = parts\n",
    "                    target = sampled_schema_groups[database][database2][\"grouped\"][table][int(index)]\n",
    "                else:\n",
    "                    print(f\"⚠️ Unknown custom_id format: {custom_id}\")\n",
    "                    continue\n",
    "\n",
    "                target[\"description\"] = description\n",
    "                target[\"keywords\"] = keywords\n",
    "\n",
    "            except (KeyError, IndexError) as e:\n",
    "                print(f\"❗ Failed to update {custom_id}: {e}\")\n",
    "\n",
    "    return sampled_schema_groups\n",
    "\n",
    "\n",
    "# Load the sampled_schema_groups\n",
    "with open(\"1_sampled_schema_groups_with_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sampled_schema_groups = json.load(f)\n",
    "\n",
    "# Inject metadata from both files\n",
    "input_files = [\n",
    "    \"/Users/paolocadei/Documents/Masters/Thesis/Spider2/1_merged_openai_output.jsonl\",\n",
    "    \"/Users/paolocadei/Documents/Masters/Thesis/Spider2/1_manually_recovered_prompts.jsonl\"\n",
    "]\n",
    "\n",
    "for file_path in input_files:\n",
    "    sampled_schema_groups = inject_metadata_from_file(file_path, sampled_schema_groups)\n",
    "\n",
    "# Save the updated version\n",
    "with open(\"1_sampled_schema_groups_with_metadata.json\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    json.dump(sampled_schema_groups, out_f, indent=2)\n",
    "\n",
    "print(\"✅ Metadata from both files injected successfully into sampled_schema_groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7d295",
   "metadata": {},
   "source": [
    "## Final Checking\n",
    "\n",
    "Checks if:\n",
    "- any table descriptions are missing\n",
    "- any table instances are missing compared to the folder structure in databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/paolocadei/Documents/Masters/Thesis/Spider2/1_sampled_schema_groups_with_metadata.json') as f:\n",
    "\n",
    "    data = dict(json.load(f))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for database in data.keys():\n",
    "\n",
    "    for table in data[database].keys():\n",
    "\n",
    "        for template in data[database][table]['grouped'].keys():\n",
    "\n",
    "            for t in range(len(data[database][table]['grouped'][template])):\n",
    "\n",
    "                # Assign to your nested structure\n",
    "                if 'description' not in data[database][table]['grouped'][template][t]:\n",
    "                     \n",
    "                     count +=1\n",
    "                     print(database, table, template, t)\n",
    "\n",
    "        for t in data[database][table]['ungrouped']:\n",
    "\n",
    "            if 'description' not in data[database][table]['ungrouped'][t]:\n",
    "                print(database, table, t)\n",
    "                count +=1\n",
    "\n",
    "print(count)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b64681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_generated_paths(nested):\n",
    "    all_paths = []\n",
    "\n",
    "    for database, tables in nested.items():\n",
    "        for table, content in tables.items():\n",
    "            # Grouped files\n",
    "            grouped_entries = content.get(\"grouped\", {})\n",
    "            for template, template_entries in grouped_entries.items():  # template_entries is now a list\n",
    "                for entry in template_entries:\n",
    "                    combinations = entry[\"combinations\"]\n",
    "                    variable_order = entry[\"variable_order\"]\n",
    "\n",
    "                    for combo in combinations:\n",
    "                        subs = dict(zip(variable_order, combo))\n",
    "\n",
    "                        try:\n",
    "                            filename = template.format(**subs)\n",
    "                            full_path = f\"{database}/{table}/{filename}\"\n",
    "                            all_paths.append(full_path)\n",
    "                        except KeyError as e:\n",
    "                            print(f\"❌ Missing key in format for {database}/{table} with template '{template}'\")\n",
    "                            print(f\"   combo: {combo}\")\n",
    "                            print(f\"   subs: {subs}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"❌ Error formatting template '{template}' with combo {combo}: {e}\")\n",
    "\n",
    "            # Ungrouped files\n",
    "            ungrouped_files = content.get(\"ungrouped\", [])\n",
    "            for filename in ungrouped_files:\n",
    "                full_path = f\"{database}/{table}/{filename}\"\n",
    "                all_paths.append(full_path)\n",
    "\n",
    "    return all_paths\n",
    "\n",
    "\n",
    "\n",
    "final_paths = extract_all_generated_paths(data)\n",
    "\n",
    "all_file_paths = final_paths #+ unique_paths\n",
    "\n",
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/0_filesystem_structure.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "ground_truth_paths = [f'{k1}/{k2}/{v}' for k1 in ground_truth.keys() for k2 in ground_truth[k1].keys() for v in ground_truth[k1][k2].keys()]\n",
    "\n",
    "if set() == set(ground_truth_paths) - set(all_file_paths):\n",
    "    print('All the tables are there!')\n",
    "else:\n",
    "    print(f'Missing: f{len(set(ground_truth_paths) - set(all_file_paths))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
