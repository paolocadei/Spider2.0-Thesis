{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import QdrantClient, models\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import uuid\n",
    "import time\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from qdrant_client import QdrantClient, models\n",
    "from openai import OpenAI, RateLimitError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import PointStruct\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1241424",
   "metadata": {},
   "source": [
    "### Importing necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "193f3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/api_keys/api_keys.json\", \"r\") as f:\n",
    "    api_keys = dict(json.load(f))\n",
    "\n",
    "qdrant_key =  api_keys[\"qdrant_key\"]\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://17582e9e-4a04-4068-bf13-bd4fdc0d688d.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=qdrant_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e508fec",
   "metadata": {},
   "source": [
    "The different embeddings and information are stored in the following files:\n",
    "- new_embeddings.json $\\rightarrow$ stores the 1536 dimensions\n",
    "- bm25_embedddings.json.gz $\\rightarrow$ stores the sparse embeddings made using BM25\n",
    "- final_structure_with_descriptions $\\rightarrow$ stores all the other information that goes in the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed9e3f",
   "metadata": {},
   "source": [
    "### Creating the Qdrant collection\n",
    "\n",
    "Here we consider 3 different types of embeddings:\n",
    "- dense column embeddings\n",
    "- dense table embeddings\n",
    "- sparse bm25 column embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddae57ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qdrant collection 'thesis' created with dense_dim = 1536\n"
     ]
    }
   ],
   "source": [
    "dense_dim = 1536  # for text-embedding-3-small (OpenAI)\n",
    "\n",
    "try:\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=\"thesis\",\n",
    "        vectors_config={\n",
    "            \"openai_column\": models.VectorParams(\n",
    "                size=1536,  # or your dim\n",
    "                distance=models.Distance.COSINE\n",
    "            ),\n",
    "            \"openai_table\": models.VectorParams(\n",
    "                size=1536,\n",
    "                distance=models.Distance.COSINE\n",
    "            )\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            \"bm25\": models.SparseVectorParams(\n",
    "                modifier=models.Modifier.IDF\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"✅ Qdrant collection 'thesis' created with dense_dim =\", dense_dim)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdada4b",
   "metadata": {},
   "source": [
    "### Uploading the Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b715ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Loaded table embeddings.\n",
      "\n",
      "✅ Loaded column embeddings.\n",
      "\n",
      "✅ Loaded sparse embeddings.\n",
      "\n",
      "✅ Loaded metadata.\n",
      "\n",
      "✅ Loaded all files. The Qdrant upload can be started.\n"
     ]
    }
   ],
   "source": [
    "# === FILE PATHS ===\n",
    "TABLE_EMBEDDINGS_PATH = \"3_table_embeddings.json\"\n",
    "COLUMN_EMBEDDINGS_PATH = \"3_dense_embeddings.json\"\n",
    "BM25_PATH = \"4_bm25_embeddings.json.gz\"\n",
    "PAYLOAD_PATH = \"2_final_structure_all.json\"\n",
    "\n",
    "# === LOAD FILES ===\n",
    "with open(TABLE_EMBEDDINGS_PATH) as f:\n",
    "    table_embeddings = json.load(f)\n",
    "\n",
    "print(\"\\n✅ Loaded table embeddings.\")\n",
    "\n",
    "with open(COLUMN_EMBEDDINGS_PATH) as f:\n",
    "    column_embeddings = json.load(f)\n",
    "\n",
    "print(\"\\n✅ Loaded column embeddings.\")\n",
    "\n",
    "with gzip.open(BM25_PATH, 'rt') as f:\n",
    "    sparse_embeddings = json.load(f)\n",
    "\n",
    "print(\"\\n✅ Loaded sparse embeddings.\")\n",
    "\n",
    "with open(PAYLOAD_PATH) as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"\\n✅ Loaded metadata.\")\n",
    "\n",
    "# === INIT QDRANT ===\n",
    "print(\"\\n✅ Loaded all files. The Qdrant upload can be started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955637c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "COLLECTION_NAME = \"thesis\"\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def upload_batch(batch_points):\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=batch_points\n",
    "    )\n",
    "\n",
    "def get_payload_hash(payload):\n",
    "    return hashlib.md5(json.dumps(payload, sort_keys=True).encode('utf-8')).hexdigest()\n",
    "\n",
    "def get_uid(database, table, column_name, group=None, ungrouped_key=None):\n",
    "    base = {\n",
    "        \"database\": database,\n",
    "        \"table\": table,\n",
    "        \"column\": column_name,\n",
    "        \"group\": group,\n",
    "        \"ungrouped_key\": ungrouped_key,\n",
    "    }\n",
    "    key = json.dumps(base, sort_keys=True)\n",
    "    return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_existing_ids():\n",
    "    existing_ids = set()\n",
    "    scroll = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        limit=10000,\n",
    "        with_payload=False,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    while True:\n",
    "        existing_ids.update([point.id for point in scroll[0]])\n",
    "        if scroll[1] is None:\n",
    "            break\n",
    "        scroll = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            limit=10000,\n",
    "            offset=scroll[1],\n",
    "            with_payload=False,\n",
    "            with_vectors=False\n",
    "        )\n",
    "    return existing_ids\n",
    "\n",
    "def upload_embeddings(metadata, table_embeddings, column_embeddings, sparse_embeddings, test_limit=None):\n",
    "    total_inserted = 0\n",
    "    points = []\n",
    "    seen_payloads = set()\n",
    "    existing_ids = get_existing_ids()\n",
    "\n",
    "    planned_points = 0\n",
    "    for db in metadata.values():\n",
    "        for tbl in db.values():\n",
    "            for group_entries in tbl.get(\"grouped\", {}).values():\n",
    "                for entry in group_entries:\n",
    "                    planned_points += len(entry.get(\"details\", {}).get(\"columns\", {}))\n",
    "            for entry in tbl.get(\"ungrouped\", {}).values():\n",
    "                planned_points += len(entry.get(\"details\", {}).get(\"columns\", {}))\n",
    "\n",
    "    pbar = tqdm(total=planned_points, desc=\"Uploading missing points\", dynamic_ncols=False)\n",
    "\n",
    "    for database in metadata:\n",
    "        for table in metadata[database]:\n",
    "\n",
    "            for template in metadata[database][table].get('grouped', {}):\n",
    "                entries = metadata[database][table]['grouped'][template]\n",
    "\n",
    "                if 'table_embedding' not in table_embeddings.get(database, {}).get(table, {}).get('grouped', {}).get(template, {}):\n",
    "                    continue\n",
    "\n",
    "                table_embedding = table_embeddings[database][table]['grouped'][template]['table_embedding']\n",
    "\n",
    "                for index, entry in enumerate(entries):\n",
    "                    grouped_cols = entry['details']['columns']\n",
    "                    col_embed_entry = column_embeddings.get(database, {}).get(table, {}).get('grouped', {}).get(template, [])\n",
    "                    sparse_embed_entry = sparse_embeddings.get(database, {}).get(table, {}).get('grouped', {}).get(template, [])\n",
    "\n",
    "                    if index >= len(col_embed_entry) or index >= len(sparse_embed_entry):\n",
    "                        continue\n",
    "\n",
    "                    col_embeds = col_embed_entry[index]['details']['column_embeddings']\n",
    "                    sparse_embeds = sparse_embed_entry[index]['details']['column_embeddings']\n",
    "\n",
    "                    table_payload = {\n",
    "                        'database': database,\n",
    "                        'table': table,\n",
    "                        'template': template,\n",
    "                        'variables': entry['variables'],\n",
    "                        'combinations': entry['combinations'],\n",
    "                        'table_keywords': entry['keywords'],\n",
    "                        'table_description': entry['description']\n",
    "                    }\n",
    "\n",
    "                    for col in grouped_cols:\n",
    "                        uid = get_uid(database, table, col, group=template)\n",
    "                        pbar.update(1)\n",
    "\n",
    "                        if test_limit is not None and total_inserted >= test_limit:\n",
    "                            if points:\n",
    "                                upload_batch(points)\n",
    "                            pbar.close()\n",
    "                            tqdm.write(f\"\\U0001f9ea Test limit reached: {total_inserted} points uploaded.\")\n",
    "                            return\n",
    "\n",
    "                        if col not in col_embeds or col not in sparse_embeds:\n",
    "                            continue\n",
    "\n",
    "                        if uid in existing_ids:\n",
    "                            continue\n",
    "\n",
    "                        col_embedding = col_embeds[col]\n",
    "                        column_sparse_embeddings = sparse_embeds[col]\n",
    "\n",
    "                        col_payload = {\n",
    "                            'column_name': col,\n",
    "                            'column_type': grouped_cols[col],\n",
    "                            'description': entry['details']['description'][col],\n",
    "                            'sample_values': [row[col] for row in entry['details']['sample_row']] if entry['details'].get('sample_row') else [],\n",
    "                            'column_keywords': entry['details']['keywords'].get(col, [])\n",
    "                        }\n",
    "\n",
    "                        full_payload = {**table_payload, **col_payload}\n",
    "                        payload_hash = get_payload_hash(full_payload)\n",
    "\n",
    "                        if payload_hash in seen_payloads:\n",
    "                            continue\n",
    "\n",
    "                        seen_payloads.add(payload_hash)\n",
    "\n",
    "                        combined_embedding = (\n",
    "                            0.3 * np.array(table_embedding) +\n",
    "                            0.7 * np.array(col_embedding)\n",
    "                        ).tolist()\n",
    "\n",
    "                        vectors = {\n",
    "                            \"openai_combined\": combined_embedding,\n",
    "                            \"bm25\": models.SparseVector(\n",
    "                                indices=column_sparse_embeddings[\"indices\"],\n",
    "                                values=column_sparse_embeddings[\"values\"]\n",
    "                            )\n",
    "                        }\n",
    "\n",
    "                        points.append(PointStruct(id=uid, vector=vectors, payload=full_payload))\n",
    "                        total_inserted += 1\n",
    "\n",
    "                        if len(points) == BATCH_SIZE:\n",
    "                            upload_batch(points)\n",
    "                            points = []\n",
    "\n",
    "            for key, entry in metadata[database][table].get('ungrouped', {}).items():\n",
    "                if 'table_embedding' not in table_embeddings.get(database, {}).get(table, {}).get('ungrouped', {}).get(key, {}):\n",
    "                    continue\n",
    "\n",
    "                table_embedding = table_embeddings[database][table]['ungrouped'][key]['table_embedding']\n",
    "                col_embeds = column_embeddings.get(database, {}).get(table, {}).get('ungrouped', {}).get(key, {}).get('details', {}).get('column_embeddings', {})\n",
    "                sparse_embeds = sparse_embeddings.get(database, {}).get(table, {}).get('ungrouped', {}).get(key, {}).get('details', {}).get('column_embeddings', {})\n",
    "\n",
    "                table_payload = {\n",
    "                    'database': database,\n",
    "                    'table': table,\n",
    "                    'ungrouped_key': key,\n",
    "                    'table_keywords': entry['keywords'],\n",
    "                    'table_description': entry['description']\n",
    "                }\n",
    "\n",
    "                for col in entry['details']['columns']:\n",
    "                    uid = get_uid(database, table, col, ungrouped_key=key)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if test_limit is not None and total_inserted >= test_limit:\n",
    "                        if points:\n",
    "                            upload_batch(points)\n",
    "                        pbar.close()\n",
    "                        tqdm.write(f\"\\U0001f9ea Test limit reached: {total_inserted} points uploaded.\")\n",
    "                        return\n",
    "\n",
    "                    if col not in col_embeds or col not in sparse_embeds:\n",
    "                        continue\n",
    "\n",
    "                    if uid in existing_ids:\n",
    "                        continue\n",
    "\n",
    "                    col_embedding = col_embeds[col]\n",
    "                    column_sparse_embeddings = sparse_embeds[col]\n",
    "\n",
    "                    col_payload = {\n",
    "                        'column_name': col,\n",
    "                        'column_type': entry['details']['columns'][col],\n",
    "                        'description': entry['details']['description'][col],\n",
    "                        'sample_values': [row[col] for row in entry['details']['sample_row']] if entry['details'].get('sample_row') else [],\n",
    "                        'column_keywords': entry['details']['keywords'].get(col, [])\n",
    "                    }\n",
    "\n",
    "                    full_payload = {**table_payload, **col_payload}\n",
    "                    payload_hash = get_payload_hash(full_payload)\n",
    "\n",
    "                    if payload_hash in seen_payloads:\n",
    "                        continue\n",
    "\n",
    "                    seen_payloads.add(payload_hash)\n",
    "\n",
    "                    combined_embedding = (\n",
    "                        0.3 * np.array(table_embedding) +\n",
    "                        0.7 * np.array(col_embedding)\n",
    "                    ).tolist()\n",
    "\n",
    "                    vectors = {\n",
    "                        \"openai_combined\": combined_embedding,\n",
    "                        \"bm25\": models.SparseVector(\n",
    "                            indices=column_sparse_embeddings[\"indices\"],\n",
    "                            values=column_sparse_embeddings[\"values\"]\n",
    "                        )\n",
    "                    }\n",
    "\n",
    "                    points.append(PointStruct(id=uid, vector=vectors, payload=full_payload))\n",
    "                    total_inserted += 1\n",
    "\n",
    "                    if len(points) == BATCH_SIZE:\n",
    "                        upload_batch(points)\n",
    "                        points = []\n",
    "\n",
    "    if points:\n",
    "        upload_batch(points)\n",
    "\n",
    "    pbar.close()\n",
    "    tqdm.write(f\"\\u2705 All done. Total points uploaded: {total_inserted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3de42d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading missing points:  35%|███▍      | 46870/134095 [2:03:50<3:50:28,  6.31it/s]\n",
      "Uploading missing points:   2%|▏         | 2210/134095 [2:03:02<122:22:21,  3.34s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae2ee01bae446e7944d9085fafa5c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading missing points:   0%|          | 0/134095 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A                                                                            \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All done. Total points uploaded: 134095\n"
     ]
    }
   ],
   "source": [
    "upload_embeddings(\n",
    "    metadata=metadata,\n",
    "    table_embeddings=table_embeddings,\n",
    "    column_embeddings=column_embeddings,\n",
    "    sparse_embeddings=sparse_embeddings,\n",
    "    test_limit=None  # ⬅️ Upload only 20 points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27e50e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points in 'thesis': 128249\n"
     ]
    }
   ],
   "source": [
    "actual_count = qdrant_client.count(COLLECTION_NAME, exact=True).count\n",
    "\n",
    "print(f\"Total points in 'thesis': {actual_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed38ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "COLLECTION_NAME = \"thesis\"\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def upload_batch(batch_points):\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=batch_points\n",
    "    )\n",
    "\n",
    "def get_payload_hash(payload):\n",
    "    return hashlib.md5(json.dumps(payload, sort_keys=True).encode('utf-8')).hexdigest()\n",
    "\n",
    "def get_uid(database, table, column_name, group=None, ungrouped_key=None, index=None):\n",
    "    base = {\n",
    "        \"database\": database,\n",
    "        \"table\": table,\n",
    "        \"column\": column_name,\n",
    "        \"group\": group,\n",
    "        \"ungrouped_key\": ungrouped_key,\n",
    "        \"index\": index,\n",
    "    }\n",
    "    key = json.dumps(base, sort_keys=True)\n",
    "    return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_existing_ids():\n",
    "    existing_ids = set()\n",
    "    scroll = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        limit=10000,\n",
    "        with_payload=False,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    while True:\n",
    "        existing_ids.update([point.id for point in scroll[0]])\n",
    "        if scroll[1] is None:\n",
    "            break\n",
    "        scroll = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            limit=10000,\n",
    "            offset=scroll[1],\n",
    "            with_payload=False,\n",
    "            with_vectors=False\n",
    "        )\n",
    "    return existing_ids\n",
    "\n",
    "def upload_embeddings(metadata, table_embeddings, column_embeddings, sparse_embeddings, test_limit=None):\n",
    "    total_inserted = 0\n",
    "    points = []\n",
    "    seen_payloads = set()\n",
    "    existing_ids = get_existing_ids()\n",
    "\n",
    "    planned_points = 0\n",
    "    for db in metadata.values():\n",
    "        for tbl in db.values():\n",
    "            for group_entries in tbl.get(\"grouped\", {}).values():\n",
    "                for entry in group_entries:\n",
    "                    planned_points += len(entry.get(\"details\", {}).get(\"columns\", {}))\n",
    "            for entry in tbl.get(\"ungrouped\", {}).values():\n",
    "                planned_points += len(entry.get(\"details\", {}).get(\"columns\", {}))\n",
    "\n",
    "    pbar = tqdm(total=planned_points, desc=\"Uploading missing points\", dynamic_ncols=False)\n",
    "\n",
    "    for database in metadata:\n",
    "        for table in metadata[database]:\n",
    "\n",
    "            for template in metadata[database][table].get('grouped', {}):\n",
    "                entries = metadata[database][table]['grouped'][template]\n",
    "\n",
    "                if 'table_embedding' not in table_embeddings.get(database, {}).get(table, {}).get('grouped', {}).get(template, {}):\n",
    "                    continue\n",
    "\n",
    "                table_embedding = table_embeddings[database][table]['grouped'][template]['table_embedding']\n",
    "\n",
    "                for index, entry in enumerate(entries):\n",
    "                    grouped_cols = entry['details']['columns']\n",
    "                    col_embed_entry = column_embeddings.get(database, {}).get(table, {}).get('grouped', {}).get(template, [])\n",
    "                    sparse_embed_entry = sparse_embeddings.get(database, {}).get(table, {}).get('grouped', {}).get(template, [])\n",
    "\n",
    "                    if index >= len(col_embed_entry) or index >= len(sparse_embed_entry):\n",
    "                        continue\n",
    "\n",
    "                    col_embeds = col_embed_entry[index]['details']['column_embeddings']\n",
    "                    sparse_embeds = sparse_embed_entry[index]['details']['column_embeddings']\n",
    "\n",
    "                    table_payload = {\n",
    "                        'database': database,\n",
    "                        'table': table,\n",
    "                        'template': template,\n",
    "                        'variables': entry['variables'],\n",
    "                        'combinations': entry['combinations'],\n",
    "                        'table_keywords': entry['keywords'],\n",
    "                        'table_description': entry['description']\n",
    "                    }\n",
    "\n",
    "                    for col in grouped_cols:\n",
    "                        uid = get_uid(database, table, col, group=template, index=index)\n",
    "                        pbar.update(1)\n",
    "\n",
    "                        if test_limit is not None and total_inserted >= test_limit:\n",
    "                            if points:\n",
    "                                upload_batch(points)\n",
    "                            pbar.close()\n",
    "                            tqdm.write(f\"\\U0001f9ea Test limit reached: {total_inserted} points uploaded.\")\n",
    "                            return\n",
    "\n",
    "                        if col not in col_embeds or col not in sparse_embeds:\n",
    "                            continue\n",
    "\n",
    "                        if uid in existing_ids:\n",
    "                            continue\n",
    "\n",
    "                        col_embedding = col_embeds[col]\n",
    "                        column_sparse_embeddings = sparse_embeds[col]\n",
    "\n",
    "                        col_payload = {\n",
    "                            'column_name': col,\n",
    "                            'column_type': grouped_cols[col],\n",
    "                            'description': entry['details']['description'][col],\n",
    "                            'sample_values': [row[col] for row in entry['details']['sample_row']] if entry['details'].get('sample_row') else [],\n",
    "                            'column_keywords': entry['details']['keywords'].get(col, [])\n",
    "                        }\n",
    "\n",
    "                        full_payload = {**table_payload, **col_payload}\n",
    "                        payload_hash = get_payload_hash(full_payload)\n",
    "\n",
    "                        if payload_hash in seen_payloads:\n",
    "                            continue\n",
    "\n",
    "                        seen_payloads.add(payload_hash)\n",
    "\n",
    "                        combined_embedding = (\n",
    "                            0.3 * np.array(table_embedding) +\n",
    "                            0.7 * np.array(col_embedding)\n",
    "                        ).tolist()\n",
    "\n",
    "                        vectors = {\n",
    "                            \"openai_combined\": combined_embedding,\n",
    "                            \"bm25\": models.SparseVector(\n",
    "                                indices=column_sparse_embeddings[\"indices\"],\n",
    "                                values=column_sparse_embeddings[\"values\"]\n",
    "                            )\n",
    "                        }\n",
    "\n",
    "                        points.append(PointStruct(id=uid, vector=vectors, payload=full_payload))\n",
    "                        total_inserted += 1\n",
    "\n",
    "                        if len(points) == BATCH_SIZE:\n",
    "                            upload_batch(points)\n",
    "                            points = []\n",
    "\n",
    "            for key, entry in metadata[database][table].get('ungrouped', {}).items():\n",
    "                if 'table_embedding' not in table_embeddings.get(database, {}).get(table, {}).get('ungrouped', {}).get(key, {}):\n",
    "                    continue\n",
    "\n",
    "                table_embedding = table_embeddings[database][table]['ungrouped'][key]['table_embedding']\n",
    "                col_embeds = column_embeddings.get(database, {}).get(table, {}).get('ungrouped', {}).get(key, {}).get('details', {}).get('column_embeddings', {})\n",
    "                sparse_embeds = sparse_embeddings.get(database, {}).get(table, {}).get('ungrouped', {}).get(key, {}).get('details', {}).get('column_embeddings', {})\n",
    "\n",
    "                table_payload = {\n",
    "                    'database': database,\n",
    "                    'table': table,\n",
    "                    'ungrouped_key': key,\n",
    "                    'table_keywords': entry['keywords'],\n",
    "                    'table_description': entry['description']\n",
    "                }\n",
    "\n",
    "                for idx, col in enumerate(entry['details']['columns']):\n",
    "                    uid = get_uid(database, table, col, ungrouped_key=key, index=idx)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if test_limit is not None and total_inserted >= test_limit:\n",
    "                        if points:\n",
    "                            upload_batch(points)\n",
    "                        pbar.close()\n",
    "                        tqdm.write(f\"\\U0001f9ea Test limit reached: {total_inserted} points uploaded.\")\n",
    "                        return\n",
    "\n",
    "                    if col not in col_embeds or col not in sparse_embeds:\n",
    "                        continue\n",
    "\n",
    "                    if uid in existing_ids:\n",
    "                        continue\n",
    "\n",
    "                    col_embedding = col_embeds[col]\n",
    "                    column_sparse_embeddings = sparse_embeds[col]\n",
    "\n",
    "                    col_payload = {\n",
    "                        'column_name': col,\n",
    "                        'column_type': entry['details']['columns'][col],\n",
    "                        'description': entry['details']['description'][col],\n",
    "                        'sample_values': [row[col] for row in entry['details']['sample_row']] if entry['details'].get('sample_row') else [],\n",
    "                        'column_keywords': entry['details']['keywords'].get(col, [])\n",
    "                    }\n",
    "\n",
    "                    full_payload = {**table_payload, **col_payload}\n",
    "                    payload_hash = get_payload_hash(full_payload)\n",
    "\n",
    "                    if payload_hash in seen_payloads:\n",
    "                        continue\n",
    "\n",
    "                    seen_payloads.add(payload_hash)\n",
    "\n",
    "                    combined_embedding = (\n",
    "                        0.3 * np.array(table_embedding) +\n",
    "                        0.7 * np.array(col_embedding)\n",
    "                    ).tolist()\n",
    "\n",
    "                    vectors = {\n",
    "                        \"openai_combined\": combined_embedding,\n",
    "                        \"bm25\": models.SparseVector(\n",
    "                            indices=column_sparse_embeddings[\"indices\"],\n",
    "                            values=column_sparse_embeddings[\"values\"]\n",
    "                        )\n",
    "                    }\n",
    "\n",
    "                    points.append(PointStruct(id=uid, vector=vectors, payload=full_payload))\n",
    "                    total_inserted += 1\n",
    "\n",
    "                    if len(points) == BATCH_SIZE:\n",
    "                        upload_batch(points)\n",
    "                        points = []\n",
    "\n",
    "    if points:\n",
    "        upload_batch(points)\n",
    "\n",
    "    pbar.close()\n",
    "    tqdm.write(f\"\\u2705 All done. Total points uploaded: {total_inserted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c577273a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71280e1add8742cfb0a7ba056198139e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading missing points:   0%|          | 0/134095 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All done. Total points uploaded: 134095\n"
     ]
    }
   ],
   "source": [
    "upload_embeddings(\n",
    "    metadata=metadata,\n",
    "    table_embeddings=table_embeddings,\n",
    "    column_embeddings=column_embeddings,\n",
    "    sparse_embeddings=sparse_embeddings,\n",
    "    test_limit=None  # ⬅️ Upload only 20 points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b314893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
