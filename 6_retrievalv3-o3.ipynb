{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d670895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pprint\n",
    "from itertools import product, islice\n",
    "import tiktoken\n",
    "import json\n",
    "import hashlib\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from tqdm.asyncio import tqdm as tqdm_async\n",
    "import json\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import QdrantClient, models\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import uuid\n",
    "import time\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from qdrant_client import QdrantClient, models\n",
    "from openai import OpenAI, RateLimitError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import PointStruct\n",
    "import cohere\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import snowflake.connector\n",
    "import re\n",
    "import traceback\n",
    "import os\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "import snowflake.connector\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from helpers import open_ai_call, clean_and_parse_response, get_embedding, prepare_embeddings_and_keywords, get_qdrant_filters, boost_points, rerank_points_with_cohere, accumulate_qdrant_points, extract_top_points_info, build_prompt_from_points, clean_sql_query, extract_one_table_per_group, fetch_column_names, load_thread_cache, save_thread_cache, get_or_create_threads_for_db, create_prompt, create_new_threads, try_extract_sql, get_assistant_response, execute_sql_with_timeout, create_assistant, inject_sample_values_into_top_points, fetch_sample_values_from_column_lookup, normalize_column_names_with_lookup, get_column_names, fetch_sample_for_column, fetch_sample_values_from_column_lookup_parallel, fetch_columns_for_table, fetch_column_names_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_sample_values_into_top_points(top_points, sample_lookup):\n",
    "    for item in top_points:\n",
    "        db = item.get('database')\n",
    "        schema = item.get('table')\n",
    "        if not db or not schema:\n",
    "            continue\n",
    "\n",
    "        # Resolve table name\n",
    "        if item.get('ungrouped_key') and item['ungrouped_key'] != 'N/A':\n",
    "            table = item['ungrouped_key'].removesuffix('.json')\n",
    "        else:\n",
    "            template = item.get('template', '').removesuffix('.json')\n",
    "            combinations = item.get('combinations', [])\n",
    "            if not template or not combinations:\n",
    "                continue\n",
    "            first_combo = combinations[0]\n",
    "            var_names = re.findall(r\"\\{(.*?)\\}\", template)\n",
    "            if len(first_combo) != len(var_names):\n",
    "                continue\n",
    "            for var, val in zip(var_names, first_combo):\n",
    "                template = template.replace(f\"{{{var}}}\", val)\n",
    "            table = template\n",
    "\n",
    "        lookup_key = (db.upper(), schema.upper(), table.upper())\n",
    "        col = item.get('column_name')\n",
    "        if not col:\n",
    "            continue\n",
    "\n",
    "        columns_dict = sample_lookup.get(lookup_key, {})\n",
    "\n",
    "        '''print(f\"\\nüîç Checking table {lookup_key} for column '{col}'\")\n",
    "        print(f\"üß© Available columns: {list(columns_dict.keys())}\")\n",
    "        print(f\"üîç Normalized: {[k.lower().strip() for k in columns_dict.keys()]}\")'''\n",
    "\n",
    "        matched_sample = columns_dict.get(col)\n",
    "\n",
    "        if matched_sample:\n",
    "            item['sample_values'] = matched_sample\n",
    "        else:\n",
    "            item['sample_values'] = 'N/A'\n",
    "            #print(f\"‚ö†Ô∏è No match for {lookup_key} ‚Üí column '{col}'\")\n",
    "\n",
    "    return top_points\n",
    "\n",
    "def fetch_sample_for_column(conn_factory, db, schema, table, col, max_samples):\n",
    "    try:\n",
    "        conn = conn_factory()\n",
    "        cs = conn.cursor()\n",
    "        query = f'''\n",
    "            SELECT DISTINCT \"{col}\"\n",
    "            FROM {db}.{schema}.{table}\n",
    "            WHERE \"{col}\" IS NOT NULL\n",
    "            LIMIT {max_samples}\n",
    "        '''\n",
    "        cs.execute(query)\n",
    "        values = [row[0] for row in cs.fetchall() if row[0] is not None]\n",
    "        cs.close()\n",
    "        conn.close()\n",
    "        return ((db, schema, table, col), values)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to fetch values for {db}.{schema}.{table}.{col} ‚Üí {e}\")\n",
    "        return ((db, schema, table, col), [])\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_sample_values_from_column_lookup_parallel(column_lookup, conn_factory, max_samples=10, max_workers=10):\n",
    "    futures = []\n",
    "    sample_lookup = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for (db, schema, table), columns in column_lookup.items():\n",
    "            for col in columns:\n",
    "                futures.append(\n",
    "                    executor.submit(fetch_sample_for_column, conn_factory, db, schema, table, col, max_samples)\n",
    "                )\n",
    "\n",
    "        # Wrap as_completed with tqdm for progress bar\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching samples\"):\n",
    "            (db, schema, table, col), values = future.result()\n",
    "            key = (db, schema, table)\n",
    "            if values:\n",
    "                if key not in sample_lookup:\n",
    "                    sample_lookup[key] = {}\n",
    "                sample_lookup[key][col] = values\n",
    "\n",
    "    return sample_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af85740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/api_keys/api_keys.json\", \"r\") as f:\n",
    "    api_keys = dict(json.load(f))\n",
    "\n",
    "client = OpenAI(\n",
    "        api_key = api_keys[\"open_ai_key\"],\n",
    "    )\n",
    "\n",
    "qdrant_key =  api_keys[\"qdrant_key\"]\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://17582e9e-4a04-4068-bf13-bd4fdc0d688d.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=qdrant_key\n",
    ")\n",
    "\n",
    "cohere_key = api_keys[\"cohere\"]\n",
    "    \n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "def conn_factory():\n",
    "    return snowflake.connector.connect(\n",
    "        user='pablo_ca',\n",
    "        password=api_keys[\"snowflake\"],\n",
    "        account='RSRSBDK-YDB67606'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34e6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "checker_instructions = \"\"\"\n",
    "You are simulating a user interacting with an LLM to generate a Snowflake SQL query that answers a natural language question.\n",
    "\n",
    "Your task is to evaluate the generated query and provide structured feedback. Follow this process:\n",
    "\n",
    "---\n",
    "\n",
    "1. Assess Relevance:\n",
    "- Does the query logically and semantically answer the user's original question?\n",
    "- Does it use appropriate columns, tables, and filters?\n",
    "\n",
    "2. Check Syntax:\n",
    "- Is the query valid Snowflake SQL?\n",
    "- If there are syntax errors, identify them precisely.\n",
    "- If the problem is with an INVALID IDENTIFIER, IT MOST LIKELY MEANS THAT THE COLUMN NAME WAS NOT REFERENCED CORRECTLY SO CHECK EACH COLUMN AND MAKE SURE THAT THEY ARE EXACTLY (EVEN THE CASE) AS THEY ARE IN THE INFORMATION PROVIDED.\n",
    "  A LOT OF TIMES THE INVALID QUALIFIER IS DUE TO THE COLUMN NAME NOT BEING IN BETWEEN DOUBLE QUOTES (e.g. \"column_name\" instead of column_name).\n",
    "- THE QUERIES NEED TO BE AS ROBUST AS POSSIBLE\n",
    "\n",
    "---\n",
    "\n",
    "3. Evaluate the Output (if present):\n",
    "- Does the result make sense given the question?\n",
    "- If the output is **null, empty, or full of zeros**, you must **never** assume the query is correct.\n",
    "  - Investigate what caused the empty or meaningless result. This could be due to:\n",
    "    - Missing or incorrect joins.\n",
    "    - Overly restrictive or wrong WHERE clauses.\n",
    "    - Invalid column references or filter logic.\n",
    "    - Improper aggregations or GROUP BY conditions.\n",
    "  - Remember: **there should always be a result**, unless the question explicitly calls for empty or filtered cases.\n",
    "\n",
    "---\n",
    "\n",
    "4. Provide Feedback:\n",
    "Write a clear, concise paragraph to send back to the LLM. It should:\n",
    "\n",
    "- Explain any issues in logic or syntax.\n",
    "- Suggest how to fix them (clearly and specifically).\n",
    "- If there are invalid identifiers, use the provided valid identifiers to correct the query.\n",
    "- If a fix is obvious, write the revised query and return it as described below.\n",
    "- If the query matches the user's intent **but returns no or zero results**, you must explain what may be causing this and request a fix.\n",
    "\n",
    "---\n",
    "\n",
    "5. Format for Output (main logic queries):\n",
    "\n",
    "If you are **confident** you can fix the original query and generate a correct one, output only the query in this format:\n",
    "\n",
    "SQL_QUERY_START\n",
    "SELECT ...\n",
    "FROM database.schema.table\n",
    "WHERE ...\n",
    "SQL_QUERY_END\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "- Only generate a new query if the previous step did **not** already include one.\n",
    "- Do **not** generate speculative or placeholder queries.\n",
    "- Do **not** assume the model has access to runtime data‚Äîonly evaluate queries and outputs as given.\n",
    "\n",
    "---\n",
    "\n",
    "6. Validate Output Columns:\n",
    "- If the expected output column names are provided, ensure the query returns **only** those columns, in the correct order and format.\n",
    "- Do not allow extra columns or missing columns.\n",
    "- If needed, use column aliases (e.g., `AS \"city_one\"`) to match expected output exactly.\n",
    "- If the output columns are incorrect, request that the query be rewritten with the correct `SELECT` clause.\n",
    "\n",
    "---\n",
    "\n",
    "7. Final Judgment:\n",
    "\n",
    "If the query is **correct** and the **output is not empty**, and both align with the user‚Äôs question:\n",
    "- Respond only with: `Query validated. Marking as final.`\n",
    "\n",
    "\n",
    "If the query looks syntactically fine but the **output is empty**, that is **not sufficient** to mark it as final. You must identify what in the query might have led to this, or provide a standalone query to assist.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sql_generator_instructions = \"\"\"\n",
    "You are a Snowflake SQL query generator. Your job is to generate a valid and efficient query using the **Snowflake SQL dialect** only.\n",
    "\n",
    "Instructions:\n",
    "- You will receive a natural language question and detailed database schema information.\n",
    "- Your task is to generate a SQL query that accurately answers the question using only the provided schema.\n",
    "\n",
    "Strict requirements:\n",
    "1. Use only **Snowflake SQL dialect** ‚Äî no functions or syntax from other dialects (e.g., no `LIMIT` without `FETCH`, no `TEXT[]`, no `ARRAY`, etc.).\n",
    "2. The FROM clause must have three parts: `database.schema.table`. This information is given in the information provided as TABLE:\n",
    "4. Do not assume columns or data unless explicitly described in the schema.\n",
    "5. If using date filters like ‚Äúone year ago‚Äù, use Snowflake syntax:\n",
    "   `WHERE refresh_date BETWEEN DATEADD(year, -1, CURRENT_DATE()) AND CURRENT_DATE()`\n",
    "6. Avoid hardcoded values when the question calls for reasoning (e.g., use calculations like midpoint/average for ranges).\n",
    "7. Never use non-SQL logic or LLM-specific language (e.g., no `ARRAY_EXISTS`, `filter()`, or pseudo-code).\n",
    "8. Use aliases and readable formatting when appropriate.\n",
    "9. If the question asks for the id and you also have a column with the name, provide both of them in the SELECT statement (HAVING MORE COLUMNS IS BETTER THAN HAVING LESS COLUMNS).\n",
    "10. MAKE SURE THAT THE COLUMNS NAMES ARE EXACTLY AS THEY ARE IN THE INFORMATION PROVIDED, IF THEY ARE IN LOWERCASE, WRAP THEM IN DOUBLE QUOTES (i.e. \"column_name\" instead of column_name).\n",
    "11. DO NOT USE TO_TIMESTAMP_NTZ(\"column_name\" / 1000000) BUT USE TO_TIMESTAMP(\"column_name\" / 1000000)\n",
    "\n",
    "MAKE SURE TO FOCUS ON THE CHANGES YOU ARE TOLD TO MAKE !\n",
    "\n",
    "THE QUERIES NEED TO BE AS ROBUST AS POSSIBLE\n",
    "\n",
    "Format:\n",
    "- Wrap the final SQL query between these tags:\n",
    "\n",
    "SQL_QUERY_START\n",
    "SELECT ...\n",
    "FROM database.schema.table\n",
    "WHERE ...\n",
    "SQL_QUERY_END\n",
    "\n",
    "Goal:\n",
    "Generate only one correct, executable Snowflake SQL query that directly answers the question using the provided schema, and that perfectly matches\n",
    "Only return the SQL query between the tags. Do not include explanations, comments, or the question itself in the response.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Your system prompt\n",
    "system_prompt = '''\n",
    "You are a natural language assistant that reformulates analytical questions for use in a retrieval-augmented SQL generation system.\n",
    "\n",
    "Your task is to take a user‚Äôs natural language question and generate 1 different reformulations that:\n",
    "- Use varied phrasing and sentence structure\n",
    "- Preserve the precise analytical intent of the original question\n",
    "- Clearly express the entities, time logic, and conditions involved\n",
    "- Are suitable for matching with table and column descriptions in a vector database\n",
    "\n",
    "Each reformulation should be accurate, well-structured, and reflect the same logical meaning as the original using synonyms and restructuring the question completely are allowed (e.g. completed orders could be completed or delivered orders...)\n",
    "\n",
    "NO TWO REFORMULATIONS SHOULD BE TOO SIMILAR. Each should be distinct in wording and structure while maintaining the original intent.\n",
    "THE REFORMULATIONS SHOULD HAVE DIFFERENT WORDING AS MUCH AS POSSIBLE.\n",
    "\n",
    "Return the output in the following JSON format:\n",
    "\n",
    "{\n",
    "\"intent_paraphrases\": [\n",
    "    \"Reformulation 1\",\n",
    "    ...\n",
    "    \"Reformulation 3\"\n",
    "]\n",
    "}\n",
    "'''\n",
    "\n",
    "system_prompt2 = '''\n",
    "You are an AI assistant designed to extract relevant entities, synonyms, metrics, time expressions, conditions, and other concepts from a user‚Äôs natural language question. These elements will be used for hybrid retrieval in a Qdrant vector database to match relevant database metadata (such as tables and columns) and assist with SQL query generation.\n",
    "\n",
    "Please analyze the user's questions and extract the following:\n",
    "\n",
    "1. **Entities**: General schema-relevant concepts implied in the question, such as \"location\", \"date\", \"timestamp\", \"category\", \"score\", etc should also be included (e.g. \"last year\" or \"in the past 5 days\" should result in \"date\" or \"timestamp\"). If any time-expressions are present, give them the label \"date\" or \"timestamp\".\n",
    "2. **Synonyms**: Alternate or related terms that a user might use for the identified entities or other key concepts, which could help match to schema elements.\n",
    "3. **Metrics**: Any quantitative concepts or aggregation methods mentioned, expressed as full phrases (e.g., \"highest search score\", \"total count\").\n",
    "4. **Time Expressions**: Literal time-based phrases from the question (e.g., \"one year ago\", \"last month\", \"from 2020 to 2023\").\n",
    "5. **Conditions**: Any filters, comparisons, or logical operations implied by the question (e.g., \"greater than 100\", \"for California only\").\n",
    "6. **Other Concepts**: Additional meaningful terms or domain-specific ideas that help clarify the user‚Äôs intent (e.g., \"top rising terms\", \"trending topics\").\n",
    "\n",
    "INPUT: A list of natural language questions, including the original and its reformulations.\n",
    "\n",
    "Return your output in this JSON format:\n",
    "{\n",
    "\"entities\": [...],\n",
    "\"synonyms\": [...],\n",
    "\"metrics\": [...],\n",
    "\"time_expressions\": [...],\n",
    "\"conditions\": [...],\n",
    "\"other_keywords\": [...]\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "040eb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    #{\"instance_id\": \"sf_bq395\", \"instruction\": \"Which 5 states' percentage change in unsheltered homeless individuals from 2015 to 2018 were top 5 closest to the national average? Please provide the state abbreviation.\", \"db_id\": \"SDOH\", \"external_knowledge\": None},\n",
    "    #{\"instance_id\": \"sf_bq352\", \"instruction\": \"Please list the average number of prenatal weeks in 2018 for counties in Wisconsin where more than 5% of the employed population had commutes of 45-59 minutes in 2017.\", \"db_id\": \"SDOH\", \"external_knowledge\": None},\n",
    "    #{\"instance_id\": \"sf_bq074\", \"instruction\": \"Count the number of counties that experienced an increase in unemployment from 2015 to 2018, using 5-year ACS data, and a decrease in dual-eligible enrollee counts between December 1, 2015, and December 1, 2018.\", \"db_id\": \"SDOH\", \"external_knowledge\": None},\n",
    "    #{\"instance_id\": \"sf_bq066\", \"instruction\": \"Could you assess the relationship between the poverty rates from the previous year's census data and the percentage of births without maternal morbidity for the years 2016 to 2018? Use only data for births where no maternal morbidity was reported and for each year, use the 5-year census data from the year before to compute the Pearson correlation coefficient\", \"db_id\": \"SDOH\", \"external_knowledge\": None},\n",
    "    {\"instance_id\": \"sf001\", \"instruction\": \"Assuming today is April 1, 2024, I would like to know the daily snowfall amounts greater than 6 inches for each U.S. postal code during the week ending after the first two full weeks of the previous year. Show the postal code, date, and snowfall amount.\", \"db_id\": \"GLOBAL_WEATHER__CLIMATE_DATA_FOR_BI\", \"external_knowledge\": None}\n",
    "\n",
    "]\n",
    "\n",
    "# Base directory\n",
    "base_path = \"/Users/paolocadei/Documents/Masters/Thesis/Spider2/spider2-snow/evaluation_suite/gold/exec_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bbf6fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " sf001 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_line(line):\n",
    "    try:\n",
    "            \n",
    "        try:\n",
    "\n",
    "            question_id = line[\"instance_id\"]\n",
    "            database = line[\"db_id\"]\n",
    "            user_prompt = line[\"instruction\"]\n",
    "            document =line[\"external_knowledge\"]\n",
    "            doc_path = f\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/spider2-snow/resource/documents/{document}\"\n",
    "\n",
    "            print(f\"\\n\\n\\n\\n\\n {question_id} \\n\\n\\n\\n\\n\")\n",
    "\n",
    "            if document is not None:\n",
    "                with open(doc_path, 'r') as f:\n",
    "                    external_knowledge = f.read()\n",
    "\n",
    "            #print('Starting the process...')\n",
    "            # Submit all 3 calls in parallel\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                future_low = executor.submit(open_ai_call, client, model, system_prompt, user_prompt)\n",
    "                #future_mid = executor.submit(open_ai_call, client, model, system_prompt, user_prompt, 500, 0.5, 0.2, 0.4)\n",
    "                #future_high = executor.submit(open_ai_call, client, model, system_prompt, user_prompt, 500, 0.9, 0.2, 0.4)\n",
    "\n",
    "                r_low_temp = future_low.result()\n",
    "                #r_mid_temp = future_mid.result()\n",
    "                #r_high_temp = future_high.result()\n",
    "\n",
    "            '''# Print the results\n",
    "            print(r_low_temp)\n",
    "            print(r_mid_temp)\n",
    "            print(r_high_temp)'''\n",
    "\n",
    "            #print(\"Reformulations received. Parsing and extracting entities...\")\n",
    "\n",
    "            info = {\n",
    "                \"entities\": [],\n",
    "                \"synonyms\": [],\n",
    "                \"metrics\": [],\n",
    "                \"time_expressions\": [],\n",
    "                \"conditions\": [],\n",
    "                \"other_keywords\": []\n",
    "            }\n",
    "\n",
    "            # First, parse all reformulations\n",
    "            parsed_reformulations = [clean_and_parse_response(r) for r in [r_low_temp]] #, r_mid_temp, r_high_temp]]\n",
    "\n",
    "            # Create all prompts beforehand\n",
    "            prompts = [\n",
    "                'Original question:\\n' + user_prompt + '\\n\\nReformulations:\\n' + '\\n'.join(r['intent_paraphrases'])\n",
    "                for r in parsed_reformulations\n",
    "            ]\n",
    "\n",
    "            # Parallel execution of open_ai_call\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = [\n",
    "                    executor.submit(open_ai_call, client, \"gpt-4.1-2025-04-14\", system_prompt2, prompt)\n",
    "                    for prompt in prompts\n",
    "                ]\n",
    "                responses = [f.result() for f in futures]\n",
    "\n",
    "            # Parse and merge responses\n",
    "            for response in responses:\n",
    "                clean_response = clean_and_parse_response(response)\n",
    "                if clean_response:\n",
    "                    for key in info:\n",
    "                        info[key].extend(clean_response.get(key, []))\n",
    "                else:\n",
    "                    print(\"Missing or invalid response structure\")\n",
    "\n",
    "            # Deduplicate\n",
    "            for key in info:\n",
    "                info[key] = list(set(info[key]))\n",
    "\n",
    "            reformulation_batches = [r_low_temp] #, r_mid_temp, r_high_temp]\n",
    "\n",
    "            # Step 1: Prepare the embeddings list and Prepare keywords from extracted info\n",
    "\n",
    "            embeddings, keywords = prepare_embeddings_and_keywords(\n",
    "                user_prompt=user_prompt,\n",
    "                reformulation_batches=reformulation_batches,\n",
    "                info=info,\n",
    "                embedding_model=EMBEDDING_MODEL,\n",
    "                openai_client=client,\n",
    "                get_embedding_func=get_embedding,\n",
    "                clean_and_parse_func=clean_and_parse_response,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Step 2: Define reusable filters\n",
    "\n",
    "            qdrant_filter_with_should, qdrant_filter_must_only = get_qdrant_filters(database, keywords)\n",
    "\n",
    "            points_sorted = accumulate_qdrant_points(\n",
    "                qdrant_client=qdrant_client,\n",
    "                collection_name='thesis',\n",
    "                embeddings=embeddings,\n",
    "                qdrant_filter_with_should=qdrant_filter_with_should,\n",
    "                qdrant_filter_must_only=qdrant_filter_must_only,\n",
    "                keywords=keywords,\n",
    "                boost_points_func=boost_points,\n",
    "                boost_factor=1.1,\n",
    "                verbose=False,\n",
    "                limit = 150\n",
    "            )\n",
    "\n",
    "            # Step 3: Run reranking using Cohere\n",
    "\n",
    "            rerank_results = rerank_points_with_cohere(\n",
    "                points_sorted=points_sorted,\n",
    "                user_prompt=user_prompt,\n",
    "                cohere_key=cohere_key,\n",
    "                top_n=75,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            top_points = extract_top_points_info(\n",
    "                rerank_results=rerank_results,\n",
    "                original_points=points_sorted,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            for item in top_points:\n",
    "                if 'column' in item and 'column_name' not in item:\n",
    "                    item['column_name'] = item['column']\n",
    "\n",
    "            # Step 1: Extract one table per group\n",
    "            table_mappings = extract_one_table_per_group(top_points)\n",
    "\n",
    "            conn = snowflake.connector.connect(\n",
    "                user='pablo_ca',\n",
    "                password=api_keys[\"snowflake\"],\n",
    "                account='RSRSBDK-YDB67606'         # Optional if your queries specify schema\n",
    "            )\n",
    "            cs = conn.cursor()\n",
    "\n",
    "            # Step 3: Query Snowflake for column names\n",
    "            column_lookup = fetch_column_names_parallel(table_mappings, conn_factory, max_workers=10)\n",
    "\n",
    "            # Step 3.5: Fetch sample values\n",
    "            sample_lookup = fetch_sample_values_from_column_lookup_parallel(column_lookup, conn_factory, max_workers=30)\n",
    "\n",
    "            # Step 2: Inject sample values after normalization\n",
    "            top_points = inject_sample_values_into_top_points(top_points, sample_lookup)\n",
    "\n",
    "            #for item in top_points:\n",
    "            #    print(f\"{item['column_name']} ‚Üí sample: {item.get('sample_values')}\")\n",
    "\n",
    "            # Getting the exact column names and format for the output\n",
    "            output_columns = get_column_names(base_path, question_id)\n",
    "\n",
    "            # Step 3: Build prompt\n",
    "            final_prompt = build_prompt_from_points(user_prompt, top_points, column_lookup=column_lookup, expected_output_columns=output_columns)\n",
    "\n",
    "            if document != None:\n",
    "                final_prompt += f\"\\n\\nThis is the external knowledge that can used to answer the question: {external_knowledge}\\n\\n IMPORTANT NOTE: The external knowledge is not a database, only use columns that have been provided above even though. It should be used only to enrich the column information above.\"\n",
    "\n",
    "            column_lookup = fetch_column_names_parallel(table_mappings, conn_factory, max_workers=10)\n",
    "\n",
    "            lines = []\n",
    "            for (db, schema, table), columns in column_lookup.items():\n",
    "                col_list = \", \".join(columns)\n",
    "                lines.append(f\"- `{db}.{schema}.{table}`: {col_list}\")\n",
    "\n",
    "            formatted_lookup = \"\\n\".join(lines)\n",
    "            final_prompt += f\"The following shows the perfect and correct tables and columns that can be found:\\n{formatted_lookup}\"\n",
    "\n",
    "            # Assuming final_prompt is a string and question_id is a string or number\n",
    "            file_path = f\"/Users/paolocadei/Documents/Masters/Thesis/Spider2/prompts/{question_id}.txt\"\n",
    "\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(final_prompt)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "        correct_path = \"/Users/paolocadei/Documents/Masters/Thesis/Spider2/7_answers_o3\"\n",
    "        wrong_path = \"/Users/paolocadei/Documents/Masters/Thesis/Spider2/7_answers_wrong_o3\"\n",
    "        middle_path = \"/Users/paolocadei/Documents/Masters/Thesis/Spider2/7_answers_middle_o3\"\n",
    "\n",
    "        count = 0\n",
    "        was_valid = False\n",
    "        last_sql_query = None\n",
    "\n",
    "        while count < 3:\n",
    "\n",
    "            '''# Always create a fresh checker thread per round\n",
    "            checker_thread = client.beta.threads.create()\n",
    "            checker_thread_id = checker_thread.id'''\n",
    "\n",
    "            print(f\"\\n=== Iteration {count + 1} ===\")\n",
    "            sql_query = None\n",
    "            results = None\n",
    "            result = None\n",
    "            checker_results = None\n",
    "            checker_sql_query = None\n",
    "            checker_answer = None\n",
    "\n",
    "\n",
    "            # ATTEMPTS TO GET A PROPERLY FORMATTED SQL QUERY IN 3 ATTEMPTS FROM THE GENERATOR ASSISTANT\n",
    "            for attempt in range(3):\n",
    "                print(f\"Attempt {attempt + 1}/3\")\n",
    "                raw_text = open_ai_call(client, 'o3-2025-04-16', system_prompt  = sql_generator_instructions, user_prompt = final_prompt, max_tokens = 1000, temperature = 0)\n",
    "\n",
    "                if not raw_text:\n",
    "                    print(\"No response from generator assistant. Retrying...\")\n",
    "                    continue\n",
    "\n",
    "                sql_query = try_extract_sql(raw_text)\n",
    "                if sql_query:\n",
    "                    print(\"Received SQL query from generator assistant:\")\n",
    "                    #print(sql_query)\n",
    "                    break\n",
    "\n",
    "                print(\"Failed to extract SQL query. Re-asking with corrected prompt.\")\n",
    "                final_prompt = create_prompt(\"Missing SQL Delimiters\", expected_output_columns=output_columns)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                print(\"\")\n",
    "                print(\"Failed to get a properly formatted SQL query after 3 attempts.\")\n",
    "\n",
    "            if not sql_query:\n",
    "                print(\"no valid sql extracted\")\n",
    "                return\n",
    "            \n",
    "\n",
    "        #### TRYING TO EXECUTE THE SQL QUERY ON SNOWFLAKE\n",
    "\n",
    "            status, results, result = execute_sql_with_timeout(cs, sql_query)\n",
    "\n",
    "            if status in (\"success\", \"timeout\"):\n",
    "                last_sql_query = sql_query\n",
    "                final_path = os.path.join(\n",
    "                    middle_path, f\"{question_id}.sql\"\n",
    "                )\n",
    "                with open(final_path, \"w\") as f:\n",
    "                    f.write(last_sql_query)\n",
    "                print(f\"Final query saved to {final_path} ('middle')\")\n",
    "\n",
    "\n",
    "\n",
    "            if status != \"error big\":\n",
    "\n",
    "                try:\n",
    "\n",
    "        #### AFTER CHECKING IF THE QUERY WAS STILL RUNNING, WE GET A QUERY FOR THE CHECKER ASSISTANT\n",
    "\n",
    "\n",
    "        #### CASE IF RESULTS WERE SUCCESSFULLY RETURNED BUT IT MIGHT HAPPEN THAT THEY ARE TOO LONG FOR THE CHECKER ASSISTANT CONTEXT TO PROCESS THEM\n",
    "\n",
    "                #### CHECKING THE OUTPUT OF THE CHECKER ASSISTANT\n",
    "\n",
    "                    checker_answer = None\n",
    "                    if status == \"success\":\n",
    "\n",
    "                        for attempt in range(3):\n",
    "\n",
    "                            print(f\"Checker attempt {attempt + 1}/3\")\n",
    "\n",
    "                            if attempt > 0 and len(results) > 1:\n",
    "                                results = results[:len(results) // 2]\n",
    "\n",
    "                            if len(result) == 0:\n",
    "                                percentage_of_results = 0.0\n",
    "                            else:\n",
    "                                percentage_of_results = len(results) / len(result) * 100\n",
    "\n",
    "\n",
    "                            content = create_prompt(status, sql_query=sql_query, results=results, question=user_prompt, percentage_of_results=percentage_of_results, expected_output_columns=output_columns)\n",
    "\n",
    "                            '''# Always create a fresh checker thread per round\n",
    "                            checker_thread = client.beta.threads.create()\n",
    "                            checker_thread_id = checker_thread.id'''\n",
    "\n",
    "                            checker_answer = open_ai_call(client, 'o3-2025-04-16', system_prompt  = checker_instructions, user_prompt = content, max_tokens = 1000, temperature = 0)\n",
    "\n",
    "                            if checker_answer:\n",
    "                                print(\"Checker assistant responded successfully.\")\n",
    "                                print(checker_answer)\n",
    "                                break\n",
    "                            else:\n",
    "                                print(\"\")\n",
    "                                print(\"No usable response from checker assistant.\")\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            # This runs if the loop completes without a break\n",
    "                            print(\"Checker assistant failed to provide useful output after 3 attempts.\")\n",
    "                            terminate = True\n",
    "                            count += 1\n",
    "\n",
    "                        ### NEED TO ADD SAVING MECHANISM FOR THE SQL QUERY AS IT WAS CORRECT\n",
    "\n",
    "                    if status == \"error\":\n",
    "                        # Check for invalid column qualifier\n",
    "                        error_message = results.lower()\n",
    "                        if \"invalid identifier\" in error_message or \"invalid column\" in error_message:\n",
    "                            content = create_prompt(\n",
    "                                \"error_with_columns\",\n",
    "                                sql_query=sql_query,\n",
    "                                results=results,\n",
    "                                question=user_prompt,\n",
    "                                column_lookup=column_lookup,\n",
    "                                expected_output_columns=output_columns\n",
    "                            )\n",
    "\n",
    "                            checker_answer = open_ai_call(client, 'o3-2025-04-16', system_prompt  = checker_instructions, user_prompt = content, max_tokens = 1000, temperature = 0)\n",
    "                        else:\n",
    "                            content = create_prompt(\n",
    "                                    status,\n",
    "                                    sql_query=sql_query,\n",
    "                                    results=results,\n",
    "                                    question=user_prompt,\n",
    "                                    expected_output_columns=output_columns\n",
    "                                )\n",
    "\n",
    "                            checker_answer = open_ai_call(client, 'o3-2025-04-16', system_prompt  = checker_instructions, user_prompt = content, max_tokens = 1000, temperature = 0)\n",
    "\n",
    "                    \n",
    "                    if status == \"empty\":\n",
    "\n",
    "                        content = create_prompt(status, sql_query=sql_query, question=user_prompt, column_lookup=column_lookup, expected_output_columns=output_columns)\n",
    "\n",
    "                        checker_answer = open_ai_call(client, 'o3-2025-04-16', system_prompt  = checker_instructions, user_prompt = content, max_tokens = 1000, temperature = 0)\n",
    "\n",
    "                    if status == \"timeout\":\n",
    "                        \n",
    "                        content = create_prompt(status, sql_query=sql_query, question=user_prompt, expected_output_columns=output_columns)\n",
    "\n",
    "                        checker_answer = open_ai_call(client, 'o3-2025-04-16', system_prompt  = checker_instructions, user_prompt = content, max_tokens = 1000, temperature = 0)\n",
    "\n",
    "\n",
    "            #### SAVING THE DIFFERENET CASES OF THE CHECKER ASSISTANT AS FINAL PROMPT FOR THE GENERATOR ASSISTANT\n",
    "\n",
    "                    print(f\"Checker assistant outputted the following response: \\n{checker_answer}.\")\n",
    "                    final_prompt = checker_answer + f\"\\nThe following shows the perfect and correct tables and columns that can be found:\\n{formatted_lookup}\"\n",
    "\n",
    "            #### CHECKING IF CHECKER ASSISTANT ANSWERED WITH A VALID SQL QUERY OR IF IT SAID \"DONE\"\n",
    "\n",
    "                    checker_sql_query = try_extract_sql(checker_answer)\n",
    "                    if checker_sql_query:\n",
    "                        print(\"Received SQL query from checker assistant:\")\n",
    "                        checker_status, checker_results, checker_result = execute_sql_with_timeout(cs, checker_sql_query)\n",
    "\n",
    "                        if checker_status in (\"success\", \"timeout\"):\n",
    "                            print(\"Checker SQL executed successfully. Saved as correct query.\")\n",
    "                            last_sql_query = checker_sql_query\n",
    "                            final_path = os.path.join(\n",
    "                                middle_path, f\"{question_id}.sql\"\n",
    "                            )\n",
    "                            with open(final_path, \"w\") as f:\n",
    "                                f.write(last_sql_query)\n",
    "                            print(f\"Final query saved to {final_path} ('middle')\")\n",
    "\n",
    "                            #### CHECKING IF THE CHECKER ASSISTANT IS SATISFIED WITH THE SQL QUERY AND SAYS \"DONE\"\n",
    "\n",
    "                            percentage_of_results = (len(checker_results) / len(checker_result) * 100) if checker_result else 0.0\n",
    "\n",
    "                            content = create_prompt(\n",
    "                                \"checker_correct\",\n",
    "                                sql_query=checker_sql_query,\n",
    "                                results=checker_results,\n",
    "                                percentage_of_results=percentage_of_results,\n",
    "                                expected_output_columns=output_columns\n",
    "                            )\n",
    "                            \n",
    "                            checker_answer = open_ai_call(client, 'o3-2025-04-16', system_prompt  = checker_instructions, user_prompt = content, max_tokens = 1000, temperature = 0)\n",
    "\n",
    "                            if checker_answer:\n",
    "\n",
    "                                final_prompt = checker_answer + f\"\\nThe following shows the perfect and correct tables and columns that can be found:\\n{formatted_lookup}\"\n",
    "\n",
    "\n",
    "                        if checker_status in (\"error\", \"empty\"):\n",
    "                            content = create_prompt(\"checker_second_attempt\", sql_query=sql_query, question=user_prompt, results = results, column_lookup=column_lookup, expected_output_columns=output_columns)\n",
    "\n",
    "                            checker_answer = open_ai_call(client, 'o3-2025-04-16', system_prompt  = checker_instructions, user_prompt = content, max_tokens = 1000, temperature = 0)\n",
    "\n",
    "                            if checker_answer:\n",
    "                                final_prompt = checker_answer + f\"\\nThe following shows the perfect and correct tables and columns that can be found:\\n{formatted_lookup}\"\n",
    "                            else:\n",
    "                                final_prompt = create_prompt(\"checker_no_second_attempt\", results = results, checker_results = checker_results, checker_sql_query=checker_sql_query, expected_output_columns=output_columns) + f\"\\nThe following shows the perfect and correct tables and columns that can be found:\\n{formatted_lookup}\"\n",
    "\n",
    "                    else:\n",
    "                        print(\"\")\n",
    "                        print(\"Checker assistant did not return a valid SQL query.\")\n",
    "\n",
    "            #### CHECKING IF THE CHECKER ASSISTANT THINKS THAT THE SQL QUERY IS CORRECT AND IS FINAL\n",
    "\n",
    "                    if \"query validated. marking as final.\" in checker_answer.lower():\n",
    "                        final_prompt = checker_answer + f\"\\nThe following shows the perfect and correct tables and columns that can be found:\\n{formatted_lookup}\"\n",
    "                        print(\"Checker marked query as correct. Saving and exiting.\")\n",
    "                        filepath = os.path.join(correct_path, f\"{question_id}.sql\")\n",
    "                        with open(filepath, \"w\") as f:\n",
    "                            f.write(last_sql_query.strip())\n",
    "                        was_valid = True\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Checker did not say 'Done'. Passing feedback to generator.\")\n",
    "                        count += 1\n",
    "                        continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è An exception occurred in checker-validation logic: {e}\")\n",
    "                    print(\"Attempting to recover using last_sql_query from middle path...\")\n",
    "\n",
    "                    if last_sql_query is not None:\n",
    "                        final_path = os.path.join(\n",
    "                            correct_path, f\"{question_id}.sql\"\n",
    "                        )\n",
    "                        with open(final_path, \"w\") as f:\n",
    "                            f.write(last_sql_query)\n",
    "                        print(f\"Query recovered from middle and saved to {final_path} (wrong)\")\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        print(\"No fallback last_sql_query available. Skipping saving.\")\n",
    "                        count += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"Some king of error occurred during the process\")\n",
    "                print(results)\n",
    "                count += 1\n",
    "\n",
    "        if last_sql_query is not None:\n",
    "            final_path = os.path.join(\n",
    "                correct_path if was_valid else wrong_path, f\"{question_id}.sql\"\n",
    "            )\n",
    "            with open(final_path, \"w\") as f:\n",
    "                f.write(last_sql_query)\n",
    "            print(f\"Final query saved to {final_path} ({'correct' if was_valid else 'wrong'})\")\n",
    "\n",
    "        print(\"Closing Snowflake connection.\")\n",
    "        cs.close()\n",
    "        conn.close()\n",
    "        print(\"Done.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing line {line.get('instance_id', '?')}: {e}\")\n",
    "\n",
    "for line in lines:\n",
    "    process_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31c5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
